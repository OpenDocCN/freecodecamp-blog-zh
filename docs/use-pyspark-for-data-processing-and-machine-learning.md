# 如何使用 PySpark 进行数据处理和机器学习

> 原文：<https://www.freecodecamp.org/news/use-pyspark-for-data-processing-and-machine-learning/>

PySpark 是 Python 中 Apache Spark 的接口。PySpark 常用于大规模数据处理和机器学习。

我们刚刚在 freeCodeCamp.org YouTube 频道上发布了一个 PySpark 速成班。

克里斯·纳伊克开发了这个课程。Krish 是首席数据科学家，他经营着一个很受欢迎的 YouTube 频道。

Apache Spark 是用 Scala 编程语言编写的。为了用 Spark 支持 Python，Apache Spark 社区发布了一个名为 PySpark 的工具。PySpark 允许人们通过一个名为 Py4j 的库在 Python 中使用弹性分布式数据集(rdd)。

以下是本课程涵盖的主题:

*   Pyspark 游戏攻略
*   Pyspark Dataframe Part 1
*   Pyspark 处理缺失值
*   Pyspark Dataframe Part 2
*   Pyspark Groupby 和聚合函数
*   Pyspark Mlib 以及安装和实施
*   数据块简介
*   在单个集群中使用数据块实现线性回归

在 freeCodeCamp.org YouTube 频道观看完整的课程(2 小时观看)。

[https://www.youtube.com/embed/_C8kWso4ne4?feature=oembed](https://www.youtube.com/embed/_C8kWso4ne4?feature=oembed)

## 副本

(自动翻译)

PiSpark 是 Python 中 Apache Spark 的接口，常用于大规模数据处理和机器学习。

Krish knack 教授这门课程。

所以我们要开始 Apache Spark 系列了。

具体来说，如果我谈论 spark，我们将关注如何将 Spark 与 Python 结合使用。

因此，我们将讨论关于称为 pi Spark 的库，我们将试图理解为什么 Spark 实际上是必需的。

而且可能还会尝试涵盖很多东西，有一个东西叫做 as 徽，spark 徽，它基本上会说你可以如何应用机器学习，你知道，在 Apache Spark 本身中，借助这个叫做 pi spark 库的 spark API。

除此之外，我们还将尝试了解未来，一旦我们了解 PI spark 库的基础知识，我们如何实际预处理我们的数据集，如何使用 PI spark 数据框架，我们还将尝试了解如何实施或使用 PI spark 和云平台，如 data、bricks、Amazon、AWS 等，我们将尝试涵盖所有这些类型的云。

记住，Apache Spark 非常方便。

让我告诉你，让我给你一些为什么 Apache Spark 非常好的理由。

因为请理解，假设您有大量数据，好吧，假设我有 64 gb 数据、128 gb 数据，您知道，我们可能有某种系统或独立系统，您知道，我们可以有 32 GB 的 ram，现在在我工作的工作站中可能有 64gb 的 RAM。

它有 64 gb 内存。

所以 max 到 Max，它可以直接上传一个 32，GB，48 gb 的数据集，对。

但是，如果我们有一个 1.8 gb 的数据集，你知道，这是一个时间，伙计们，我们不仅仅依赖于一个本地系统，我们将尝试预处理特定的数据或在分布式系统中执行任何类型的操作，对，分布式系统基本上意味着将有多个系统，你知道，在那里我们可以实际运行作业或处理流水线，或尝试做我们真正想要的任何类型的活动。

毫无疑问，Apache Spark 将帮助我们做到这一点。

这真是太神奇了。

是的，人们非常想要这种视频。

因此，我们将如何浏览这个特定的播放列表，我们将尝试，首先从安装开始，将尝试使用 PI spark，因为这也是 Apache Spark，它是 Python 的 spark API，当您实际使用 Python 时，我们基本上使用 PI Spark 库。

是的，我们也可以在其他编程语言中使用 spark，比如 Java、Scala R。

好的，我们会试着从基础开始理解，你知道，从基础开始，我们如何读取一个数据集？我们如何连接到数据源？可能，我们如何处理数据帧，你知道，在这个 Apache Spark 中，这是你的 PI Spark，而且，它们为你提供数据结构，像数据帧，这非常类似于熊猫的数据帧。

但是，是的，那里支持不同种类的操作，我们将一个接一个地看。

然后我们将尝试进入 emlid、spa、Apache Spark 和 lib。

所以基本上，它被称为 spark em lib，它将实际上帮助我们执行机器学习，在那里我们将能够执行一些机器学习算法任务，我们将能够进行回归、分类和聚类。

最后，我们将尝试了解如何在云中执行相同的操作，我将尝试向您展示一些示例，在这些示例中，我们将拥有庞大的数据集，我们将尝试在系统集群中执行操作，您知道，在分布式系统中，我们将尝试了解如何在其中使用 spark。

所以所有这些基本上都包括在内。

现在，Apache Spark 的一些优势以及它非常出名的原因，因为它运行工作负载的速度快了 100 倍，这基本上意味着，如果你们了解大数据，伙计们，当我们谈论大数据时，我们基本上是在谈论庞大的数据集。

如果你听说过这个术语叫做 MapReduce，对吧，清道夫 Apache Spark 比 MapReduce 快 100 倍。

好的，它的一些优点是易于使用。

您可以用 Java、Scala、Python 或 r 快速编写应用程序

正如我所说，我们将重点关注 Python，在这里我们将使用一个名为 pi Spark 的库。

然后你也可以结合 sequel 流和复杂的分析。

当我谈到复分析时。

我基本上说的是这个标志，机器学习库。

这肯定会与 Apache Spark 配合得很好。

Apache sparks 可以在 Hadoop 上运行，Apache missiles Cuba net 可以在我们的云中独立运行，云，不同类型的云伙计们，当我谈到 AWS 数据、砖块等所有这些东西时，我们肯定可以很好地工作。

它实际上是以集群模式运行的，集群模式基本上是指分布式模式。

这些是一些例子。

现在，如果我，如果我，关于哪个版本的 pi spark 将会产生火花，我们将会使用 pi spark 3.1 点，我们将会尝试工作。

如果你只是去搜索这里，你可以看到 SQL 和数据帧，在这里你可以看到 Spark streaming machine emilich，这就是所谓的机器学习。

好了，除此之外，如果我去看看概述，在这里，你可以看到 Apache Spark 是一个快速和通用的集群计算系统，并提供了标量，Java 和 Python 的高级 API，这使得并行作业很容易编写一个支持真正竞争图的优化引擎，它基本上是处理大量数据简而言之，你知道，这是非常方便的，我们将尝试工作。

现在，如果我在 Python 中搜索 spark，你知道，这个页面基本上会显示你是开放的。

我们会试着讨论如何安装它。

在这个视频中，我们将尝试安装 PI spark 库。

如果我说的是 pi spark 库，你会发现 pi spark 库非常神奇。

如果你真的想在现场工作，如果你想用 Python 来工作 spark 功能，你基本上使用这个特定的库。

让我们继续，让我们试着看看我们如何快速安装特定的库，并检查我们实际上可以做些什么？好的，所有这些我们将会看到的。

所以让我们开始吧，请确保你在使用 PI Spark 时创建了一个新的环境。

所以我在这里创建了一个名为 as my envy 的新环境，首先我会尝试安装 PI spark 库。

所以我就写 pip 安装 pi Spark。

好的，让我们看看，在这里，我们将重点关注安装，将重点关注读取一些数据集，并尝试查看我们实际可以做的事情，好的，在完成这些之后，我们实际可以做的是，您可以看到我们的 PI spark 已经安装，为了检查安装是否完美，我将只通过 spark 编写输入。

这看起来非常好，它在工作，你知道，我们可以看到 PI 火花基本上安装正确。

现在，你可能会面临一些关于π火花的问题。

这就是我告诉你创造一个新环境的原因。

如果您遇到了某种问题，请告诉我您遇到的错误是什么？大概是写在评论区吧。

好的，现在，让我们做一件事，我打开一个 Excel 表格。

好吧。

可能我会试着创建一些数据集，我会说名字，可能我会说名字，和年龄，对吧。

假设我的名字在这里，我写为 crash，也是 31。

我要说苏丹鞋。

好的，我会说好的，30，可能，我会写更多的名字，比如桑尼，可能，我也会给出 29 的数据。

这三个数据，我们试着看看如何读取这个特定的文件。

好吧，我就把它存起来。

让我想想，我会把它保存在我的 Jupyter 笔记本的同一个位置，伙计们，他创建了一个文件夹，我想，你可以把它保存在你的笔记本文件打开的任何位置，对不对？所以没必要。

确保你不会看到我的任何文件。

好吧，我只是在保存它。

好的，我把它保存为 test 1，你可以看到我把它保存为 test 1 . CSV。

所以我会保存它。

让我们保存这个特定的文件。

好吧。

现在，如果我可能想，你知道，和熊猫一起读，我们写的我们写 PD 点读下划线 CSV，对。

我基本上使用这个特殊的数据集，叫做 test one dot CSV。

所以当我在这里执行时，你可以看到具体的信息。

现在，当我真的想与 PI Spark 一起工作时，首先，请记住，我们需要开始 Spark 会话。

为了开始一个 spark 会话，首先，让我再创建一些字段。

只需遵循以下特定步骤，或者关于创建 pass 会话。

所以我就从 pi Spark，dot SQL，input spark session 写。

好的。

然后我将执行这个，您可以看到它显示良好，好吧，对不起，我不知道打开了什么。

所以我会写我会创建一个变量叫做火花。

我可能会使用 spark 会话点生成器。

我会说应用程序名称。

这里我只给出我的会话名。

好吧。

这样就像练习一样。

假设我在练习这个东西。

然后我可以说获取或创建。

因此，当我实际执行这个命令时，您将能够看到我们将创建树皮会话。

如果你是第一次执行，可能需要一些时间。

除此之外，如果我已经执行了多次，那么你将能够工作。

在这里，您可以清楚地看到，当您在本地执行时，它们总是只有一个群集，但当您实际在云中工作时，您可以创建多个群集和实例。

所以您将使用的 spark 版本是 V 3.1。

第一点。

在这里，您可以看到这基本上存在于主机中，当您可能在多个实例中工作时，您会看到主机和集群 1、集群 2 等所有这些类型的信息。

好的，这是关于火花的。

现在，让我们写 pi Spark 的 F，我会试着读取一个关于 Spark 的数据集。

现在，为了读取数据集，我可以写什么，我可以像这样写火花点读取点，有很多选项，如 CSV 格式，JDBC，Parque qL，模式，表格文本，有很多选项。

这里我们要取 CSV，这里我只写提示，一个，提示，一个点 CSV，对吗？如果我试着执行它，我会得到一些错误，说这个文件不存在。

让我看看。

我想这个文件是存在的。

让我看看，为什么这不是执行提示，一，二，f 文件打开。

在这里，我可以看到测试一个点 CSV，好吧，对不起，我没有写 CSV 文件，我猜，测试一个点 CSV。

好了，现在成功了。

现在如果我去看白点，圆周率火花，它显示这两串，对，这两列 C 0 和 C 1。

现在，您可以看到，我创建了这个特殊的 CSV 文件。

它可能只是把这个 A B 作为默认列。

所以它表示 C 0 和 C 1。

因此，我们实际上可以做的是，如果你真的想看到你的整个数据集，你基本上可以看到像这样的 df 下划线 pi 火花点显示你在这里，你将能够看到姓名和年龄，有这个有这个信息，我真的想让我的列名或年龄作为我的主列，对不对。

但是当我直接正确地读取 CSV 文件时，我们得到了下划线。

所以为了解决这个问题，我要做的是，我们有一个不同的技术文件，正确的火花点读点选项，有一个叫做 as 选项的东西。

在这个选项中，你基本上可以给出的是，会有一个关于头的选项，我想，看，会有类似键值的东西，你会提供一个选项。

所以你能做的，就是写标题，逗号真。

所以不管第一列第一行的值是什么，它都被认为是你的标题。

如果我写一个关于测试 1 的 CSV，现在我要读这个测试 1 数据集测试 1 点 CSV。

现在，一旦我在这里执行这个，你将会看到，我能够得到现在命名为字符串 h 的字符串，好的，但是让我们看看我们完整的数据集。

因此，如果我现在执行这个操作，我将能够看到包含这些特定列的整个数据集。

好，让我快速地把它保存在我的 df 下划线 pi Spark 中。

好，现在让我们来看看这种类型的 df 下划线 pi 火花。

现在，当我在这里执行此操作时，您将能够看到当我在阅读此 df 时，如果我在 pandas 的帮助下去查看它的类型，在这里，您将能够看到有合作伙伴或核心点帧点数据帧，但在这里您将看到当您读取此特定数据集时，它是 pi 火花点 SQL 点数据帧点数据帧类型。

是的，这就是熊猫数据框，这个 SQL 点数据框 Rhonda，是的，大多数 API 几乎是相同的，功能也是如此，我们将继续学习很多东西。

但是如果我很快想看我的，可能我不知道头是否能工作，让我看看，是的，头也能工作。

如果我用点号，你可能会看到，行信息基本上都显示在这里。

现在，如果我真的想提供关于我的列的更多信息，我将能够使用称为打印模式的东西。

好吧。

现在，在这里，这个打印模式就像一个 df 点信息，它实际上会告诉你的列，如姓名字符串和年龄字符串。

好的，所有这些都是你在安装后实际完成的一些基本操作。

同样，我关注这个问题的主要原因是，只需尝试安装这个 PI spark 并为我的下一次会议做好准备，我将尝试向您展示我们如何更改数据类型，如何处理数据框，如何实际进行数据预处理，如何处理空值、缺失值，如何删除列，如何做各种事情，所有这些事情，基本上我们将在那里讨论如何删除列。

所以伙计们，我们将继续 PI 火花系列。

在本教程中，我们实际上会看到我们的 pi spark 数据帧，我们将尝试读取数据集，检查列的数据类型，我们基本上看到了 PI spark 颜色模式，然后我们将看到如何选择列并进行索引。

我们将看到描述功能，我们有类似于熊猫，然后我们将尝试看看我们如何可以添加新的列，并可能删除列。

这只是第一部分。

让我把它作为第一部分写下来，因为在这之后，还会有一部分为什么这个视频会很重要，因为在 PI Spark 中，如果你计划应用徽，你知道，机器学习库，你真的需要在最初做数据预处理，你知道，可能在第二部分，我们会尝试看看如何处理丢失的值。

我们会试着看看如何过滤行，如何设置过滤条件，好了，让我们继续。

在继续之前，我要做的是，首先，你有一个数据集，叫做测试一。

所以我选了三个专栏。

一个是名字，年龄，经历。

然后我有一个数据集，比如 Krish Taki 110，比如这个苏丹鞋 Sunday，对。

这是一些数据集，你保存在同一个位置。

现在我首先要做的是，像往常一样，关于 pi spark 的第一步是构建 PI spark 会话。

现在，为了构建 PI spark 会话，我将逐行编写代码。

所以请确保你也跟着我做，这肯定会有帮助。

我将从 pi spark dot SQL，input，sparks session 开始写，然后我将创建一个变量，哦，对不起，然后我将开始创建一个关于我的会话的变量。

所以我会写 spark 等于 sparks session.we，基本上，就像 builder 点 app name。

在这里，我将我的应用程序命名为 practice，我可以这么说，或 liquid data frame practice 或 data frame，对，类似这样，因为我们正在练习 data frame dot get 或 create 函数。

这就是你如何真正开始一个会话。

所以再说一次，如果你是第一次执行，这将需要一些时间，否则，它会，这是完美的。

这是我的整个 Spark，它在内存中运行，在这里运行的版本。

显然，当您运行本地时，基本上只有一个主节点，应用程序名为 data frame。

首先，我们将尝试再次读取数据集。

所以我们来读一下数据集。

现在阅读数据集，我已经向你展示了多种方法。

一个是读取选项一是，因为这是一个 CSV 文件，我们将尝试先读取它第一个选项，我们将尝试如何实际读取它。

然后我会给你展示多种解读方式。

好的，我会写火花点读点选项。

在这个选项中，我们基本上说的是键值，对，所以在这里，我只是把它当作 header 等于 true，这样，你知道它应该把我的第一行当作 header。

在这里，我把它写成头，那是真正的点 CSV，在 CSV 里面。

我给我的数据集命名为 test one。

点 c 是对的。

当我执行这个的时候，我想你可能会看到数据集。

在这里，您可以看到，这是一个数据框，它将具有姓名、年龄经历等特征，对吗？所以如果我想看完整的数据集，我就写点 show。

这是我的全部数据，我可以很清楚地看到。

现在。

让我把它保存在一个名为 df 下划线 pi Spark 的变量中。

好，这是我所有的数据。

现在，第一件事，我们如何检查模式，让我们检查模式。

好的，模式基本上意味着数据类型，就像我们在熊猫 df 点信息中写的那样，同样我们可以在这里写。

所以在这里你可以看到，我已经写了 df 下划线 pi 火花点打印我认为它应该工作打印模式或没有类型有春天的主题哦，对不起。

所以我写了点显示，并保存在一个变量中，我会删除这个点显示，让我再执行一次。

现在，如果我写打印模式，在这里你将能够看到姓名，年龄和经验，但默认情况下，它是一个字符串，即使在我的 Excel 表中，我们所做的是，我们已经写了值可能这应该是字符串，这应该是整数，然后他们应该是整数，但为什么它是一个字符串。

原因是它可能需要一个字符串，因为默认情况下，除非我们在 CSV 中没有给出更多的选项，这个 CSV 有一个选项叫做推断模式，好吗？如果我不把它变成真的，对吗？默认情况下，它会考虑流值字符串值中已知的所有要素。

所以我现在就执行这个。

现在，如果我去看看 F 下划线 pi 火花点打印，Sima 你将能够看到我得到的名称和字符串年龄作为整数经验作为整数，级别等于 2，这基本上意味着它可以有空值。

好的，这是一种解读方式。

还有一种方法，我将尝试向您展示，这非常简单，因此我可以在一个东西本身中同时包含 header 和 infer schema。

所以我会写 d f 下划线 pi spark 是对 spark 点的调用读点 CSV，在 CSV 文件中首先我会提供我的测试文件 CSV 好的，然后在这里我会继续写 header 大概等于 true，我可以写推断模式等于，所以当我这样写的时候，如果我写 df 下划线 pi spark 点在这里给你看，你会看到我所有的数据集好的，这是我的整个数据集。

现在，如果我再去看看并执行这个模式，它可能会给我同样的方式，就像我们在这里一样。

所以，在这里你可以看到名字等于字符串等于整数经历贫穷、贫穷、完美。

我们已经做了什么，我们已经了解了这个，如果我去看看它的类型，如果我去看看它的类型，这基本上是一个数据框架。

熊猫也有一个数据框。

因此，如果有人在采访中问你，什么是数据框架，你基本上可以说数据框架是一种数据结构，因为在其中你可以执行各种操作。

所以这也是一种数据结构。

好，那么我们实际上做了什么，我已经介绍了数据帧读取数据集，而不是检查列的数据类型。

为了检查列的数据类型，我们已经编写了打印模式。

好了，接下来我还可以做一件事，让我们看看选择列和索引。

首先，让我们了解一下基本上有哪些列，你如何得到所有的列名。

为了得到列名，你可以写点列。

当你在这里执行的时候，你可以得到列名，比如名字，年龄经历，完美，这很好。

这是我的 D F。

现在假设如果我想获得一些头部元素，我也能够获得，因为在熊猫中你还没有，假设我想获得前三名的记录。

我会以列表的形式得到这个特殊的格式。

通常在熊猫中，当我们使用时，我们通常得到一个数据帧格式。

所以在这里你会看到名字年龄和权宜之计的结合。

好的，像这样。

这是我的第一排，这是第二排，这是第三排。

好了，接下来我们要讨论的是，如何选择一列？你知道，我可能想拿起一个专栏，看看所有的元素，就像我们在熊猫里做的那样。

首先，让我把它写成这样，如果我真的想在名字列上选择，这里的点显示将能看到所有的列。

好吧，那我该怎么做？好吧，让我们来看看。

现在，为了选择名称列，我们将编写一个非常简单的函数，称为 pi 火花点选择。

在这里，我只给出我的名字。

现在，一旦我执行了这个，您将能够看到返回类型是数据帧。

好的，返回类型是 data frame，name 基本上是一个字符串。

现在，如果我写 dot show，我将能够看到整个列。

好的，当我这样做时，我将能够看到这个，如果我试图找到它的类型，对不起，如果我移除这个点显示，看到它的类型，这基本上是一个由 spark 点 SQL 点数据框点数据，而不是熊猫点数据框。

好吧，非常简单。

现在，假设我想选择多行，比如多列，比如我想选择姓名和经历，大概是两列，我要做的就是修改其中一列。

给你。

最初，我是这样提供我的一个列名的。

在这之后，我将提供另一个类似于经验的列，现在我将执行这个。一旦我在这里执行这个，你们可以看到我得到的数据框有两个特征，一个是姓名，另一个是经验。

现在，如果我去写点显示，这里你可以看到我所有的元素，基本上都在这个特定的数据框里。

非常简单的基础，你如何选择多行？是的，这里切片肯定不行，因为我试着做切片，它不行，好吗？好吧，无论何时你有任何担心，总是试着去看文档，PI spark 文档，非常简单。

好的，这是一种方法，我们可以实际选择列，并可能看到行。

好吧。

现在，让我们展示一下，如果我只是想拿起，还有一种方法，就像，看我是否写下圆周率的 F。

如果我在这里执行，您将能够在那里看到列名。

返回类型将是这里的列，如果我直接选择的话，因为在熊猫中，我们像这样直接选择。

当我们有这种类型的列时，肯定不会，我们只是能够理解这个特定的功能，它基本上是一个列，它说，好吧，没什么，我们将无法获得数据集，将没有显示功能，它将说，它基本上是一个错误。

所以通常我们所做的是，每当我们想要获得任何类型的列并尝试查看它时，我们基本上需要使用这个特定的选择操作进行选择，这是我的功能。

好了，这些事情已经完成了，伙计们，我们现在要理解的是，让我们看看如何检查数据类型。

所以有一个功能叫做 D 类型。

所以在这里你将能够看到名字叫做字符串年龄等于结束经验等于 int。

同样，D 型也非常相似，因为我们也在熊猫身上使用。

好了，大部分功能和熊猫差不多，伙计们。

那么我们实际上做了些什么呢？让我们看看，pi spark DataFrame，读取数据集检查数据类型，选择列和索引，检查描述选项类似熊猫。

因此，我们也可以查看描述选项。

让我们看一下 pi 火花点描述，如果我执行这个，你将会看到它会给你一个数据帧摘要，等于字符串，这个，这个信息在那里。

现在，当我写点显示时，你将能够看到所有这一切，这基本上是以数据框的形式，你可能会想为什么这个空值来表示和标准划分，因为即使在这种情况下，它也会将字符串列也基本上是具有字符串数据类型的值删除，显然，你没有任何东西， 最小和最大值基本上是在索引上，因为在第二个索引的零中，你将能够看到碾压，然后在那之后，桑尼在那里，好的，寻找下一个，剩下的所有这些信息实际上再次出现。

所以这基本上和我们实际看到的描述选项一样，你知道，可能在我们的熊猫身上，所以类似地，我们实际上已经做了。

好了，描述选项也完成了。

那么，让我们来看看添加列，删除列。

所以添加列和删除列是非常非常简单的，如果我们需要添加列，我就在这里写注释，在数据框中添加列，这个数据框是 pi spark 数据框，好的，现在为了添加列，我们有一个神奇的函数 add 函数，它被称为 PI spark 点，还有一个东西被称为宽度列。

好吗？现在，如果我看到这个宽度列的功能，它会通过添加一个类或替换同名的现有列来返回一个新的数据框。

好，这里，我要给的第一个参数是我的列名。

假设我想拿起让我想想，我会拿起经验。

所以我就说经验，好吧。

这可能会成为我的新专栏。

两年后，如果经历两年后会发生什么，你知道，最初，候选人是 10 年经历，两年后，它会变成 12 年。

好的，我们现在试着输入值，这是我的新列名，以及它应该有什么值。

因此，我将写 df pi Spark。

在这里，我会说，可能，我会把同样的经验，乘以我会加上两个，因为两年后，经验会增加两个，只是我采取一个，一个实际解决这个问题的方法，我可以把任何我想要的值，这取决于你们。

好吧，你可以去看看。

好的，在这之后，只需要做两件事。

现在如果我执行它，你会看到同样的操作会发生。

现在在这个数据框中，您有 123 和 4 个特征，如果我想查看完整的数据集，我可以添加点显示，一旦我现在在这里执行它，我们将看到两年后的体验不过是 266，因为 10 加 212 您有非常非常简单的休息，这是宽度列基本上告诉我们的，您还可以就此做不同的事情。

这就是在数据框中添加一列的方法。

伙计们，这不是一个就地操作，你基本上需要把它赋给一个变量，以便得到反映。

假设如果我想得到反映，我真的需要这样赋值。

现在，如果我去看我的对不起，首先，让我把这个节目去掉。

这个节目不会给我们合适的结果。

好的，oh 没有带列的属性。

好吧抱歉。

所以，这里有一个问题，我会读这个数据集，因为我完全正确地替换了它。

现在我将执行它。

再说一次，现在很好。

现在，如果我去写点显示，在这里，你将能看到所有的元素。

现在，这是关于添加带有数据框的列。

现在，我可能也需要删除列。

所以放下柱子。

让我们看看如何实际删除这些列。

删除列非常简单，就像我们通常删除这个删除功能一样。

默认情况下，您可以给出一个列的列表。

您可以给出一个列名。

所以假设我说两年后的经历，我想放弃这个，因为谁知道，两年后，会发生什么。

让我们放下这个，按顺序，放下这个，像这样执行，然后去看点显示。

在这里，您将能够找到没有特定的列。

同样，这不是一个就地操作，你需要把它赋给一个变量，非常简单。

让我把它赋给一个变量，等于，请确保你去掉了点号，点号显示的功能，对吗？现在，如果我写这个点显示，在这里，你可以看到所有的元素。

但是现在让我们来看看如何重命名该列。

我们这么做是因为你真的需要非常擅长数据预处理，好的，我会写热函数，还有另一个函数，叫做列重命名。

好吗？九、这个你只需给出你现有的和新的列名。

假设这里有我现有的列名，我会说，name。

我会说新名字。

好了，刚刚执行。

现在，如果我像 dot show 一样，试着看看这里的元素，你将会看到一个叫做新名字的东西，而不是名字。

没错。

这才是我真正要讨论的。

我只是在这里再写一点。

我们还讨论了重命名列的问题，对吗？是的，这只是第一部分或数据帧，第二部分，我们将尝试做一些称为过滤操作。

在过滤操作中，我们将尝试了解各种操作，因为这将非常令人惊奇，您将能够学到很多东西。这可能是教程三，是关于数据框操作的第三部分。

在这个视频中，我们将看到如何处理缺失值和空值。

简而言之，我们实际上要做这么多事情，我们会看到如何删除列，如何删除行，然后我们会看到当我们删除行时，可能基于空值，我们会尝试删除一朵玫瑰。

然后，我们将尝试通过均值、中值或众数来了解丢弃功能和处理缺失值的各种参数。

好的，那么在这里，我就把它写成均值，中值，更有可能是，对。

因此，我们实际上要看到的所有这些东西，同样，主要的事情是，我真的想向您展示我们如何处理丢失的值。

这非常重要，因为在熊猫身上，我们也试图在一定范围内做到这一点，因为我们有某种内在的功能。

所以我们继续吧。

每当我们通常启动 pi Spark 时，每当我们使用 PI spark 时，我们确实需要启动一个 PI Spark 会话。

所以我希望到现在为止你们都很熟悉。

所以我要写 pi spark dot SQL，我要再次导入 sparks 会话。

然后我要用 Spark 创建一个变量。

然后这里我要写 spark session 点构建器。

不可能。

好吧，不是应用名称。

再说一遍，让我把这个应用程序的名字保持为 practice，好吗，因为我只是在练习，然后我喜欢 get 或 create 并执行它。

所以总的来说，执行起来还需要一段时间。

是的，它执行得很好。

为此，我刚刚创建了一个非常简单的数据集，看起来像这样。

我有一个像姓名、年龄、经历、薪水这样的栏目。

这些都是我的名字，所有候选人的名字，可能还有一些值是空的。

在这里，你可以看到一些我留为空白的值。

因此，我们将尝试看看如何可能删除一个空值，或者如何处理这个特定的缺失值。

好吧，那我们继续。

所以首先，为了读取数据集，我只写 spark dot read dot CSV。

在这里，我将使用 CSV 文件名来点 CSV。

它保存在这个特定文件所在的相同位置，无论如何，我也会向您提供这些数据。

我将使用 header 等于 true，可能还有 info schema 等于 true，这样我就能正确地得到数据集。

所以，当我在读这个的时候，你可能会看到这是我实际得到的数据框，如果你想看到整个数据集，这就像给我们看点显示。

这是你的整个数据集，你在完美下有空值。

让我做一件事，把它保存在一个变量里。

所以我就写 df 下划线 pi 火花。

所以如果我现在去检查，点显示，这是我的整个数据集。

好的，很好。

我们非常好，我们在这里工作得很好。

关于这个。

我们知道我们实际上也读取了一些数据集。

现在，也许首先，让我们开始。

我们如何删除这些列？放下柱子是非常非常简单的事。

假设我想删除 Name 列，那么我只需使用 df 点 drop 并像这样提供我的列名，对吗？所以列，右边的列名，假设我写 df.pi spark，这里列名将被命名为。

所以让我把它写成名字。

我基本上可以去看看我的点展。

然后，您将能够看到所有实际存在的功能。

这很简单，我可能在之前的课上也给你们展示过。

基本上就是这样做的，删除你的功能或专栏，但我们主要关注的是删除无价值的内容。

所以现在，让我只写 df.pi 火花点显示。

这是我的数据集，对吗？现在让我们看看如何删除基于空值的特定行。

所以在这里，我就用 df.by spark.na。

好的，有一种叫做 na 的东西，然后你有 drop，fill 和 replace。

所以首先我先从 dropped 说起。

现在，在这个特定的 drop 中，请记住，如果我没有给出任何东西，好的，在这里展示的，你可以看到，只要有空值，所有的行将被删除。

这里，我们将看到最后三行不存在，对吗？在这里，您可以看到 Shibam 这个特定的值存在，这意味着所有的行都被完全正确地删除了。

所以完全不是问题。

简而言之，你要做的是，无论何时你使用. na 点，它将会把这些行放到实际上没有值的地方，或者实际上有空值的地方。

如果我在 drop 中搜索，有两个主要特征一个是如何，一个是阈值，还有一个是子集。

所以，让我们试着去理解这个特殊的特征。

现在，首先，我将从 any 如何等于我刚才这样尝试的开始。

假设我写了 df.pi spark.na dot drop，如果我的 how 值可以有两个值，一个是任何一个都可以，一个是任何一个都可以，一个是任何一个如果值被选为任何 drop 一行，如果它包含任何注释，即使只有一个好的，一个丰富的调谐器，或者有一整张嘴，你知道默认情况下没有，它将被丢弃。

但是如何调用 all 呢，我们什么时候使用 all，基本意思是假设，如果在未来你有假设，如果你有任何规则，所有的值都为空，在这种情况下，你有 36 个值，这不会被删除，但如果他在一个记录中，你有所有的值和 ml，那么只有它会被删除。

让我们看看这是否可行，肯定是不可行的，因为我知道至少一个值，或者至少一个值，一个值，一个非空值，总是在那里，对吗？如果我使用 how 等于 all，它将删除那些完全没有的记录，默认情况下，How 值应该有任何权利，所以，默认情况下，它是 any any 基本上说，无论有一个还是两个，现在我们只需删除它，删除那些特定的记录。

非常简单，这是怎么回事，让我们继续，试着去理解阈值，这个阈值是什么，我会告诉你这个阈值是什么，让我用这个，好的， 我知道如何是 any，但是还有一个选项叫做 thrush nine Thresh，我们做的是，假设如果我是对的，让我们把阈值保持为 2，它基本上是说假设在这里，在这个特定的例子中，如果阈值是 2，好的，让我们首先执行它，你会看到最后一列在这里被删除了，好的， 这是最后一行被删除了，为什么会被删除，因为我们保持阈值为 2，它说至少应该有两个非空值，好的，至少有两个非空值，现在这里有两个非空值，比如超过 40，000 个，好的，这里只有一个非空值。

因此，它被删除了，假设这里有两个非空值，看 34 和 10，这没有被删除，这里也一样，如果我给你们看这里的 3410 和那里的 38000，至少这里你加了三个正常值。

这里，每当我们给它一些阈值时，它基本上会检查特定行中是否至少有两个非空值，如果有，它会保留该行，否则它会删除该行，这是你不能做的，你也可以检查一个，如果我去查看一个，那么你可以看到所有特定的行都在那里，因为它会检查，好的，这里有一个非九值，这里有。

如果我做成了三个，好吧，让我们看看会发生什么。

现在，你可以看到，至少这是它们剩余的部分，都被删除了，看这里，这里只有两个非非值，你如何加上三个，这是 3410，38，009，你可以看到这个值，它是关于阈值的。

现在，让我们继续另一个，我们称之为子集。

所以在这里我只是把它写成子集，因为这是我的 drop 特性中的第三个参数。

请记住，如果你和熊猫一起工作过，这些功能非常简单，同样的事情我们正在我们实际能提供的子集之外工作。

假设我在主题中说，让我们删除阈值，我不想保留任何阈值，假设我只想删除我的特定列的九个值，可能只来自经验列，那么我基本上可以将其作为子集。

因此，从“经验”一栏中，您可以看到记录中没有值的地方，所有保存记录的地方都被删除了。

因此，您可以这样应用，假设您想在年龄中应用它，您也可以在年龄列中没有旧记录已被删除的值的任何地方应用它。

所以这是关于子集的，所以我希望你们得到一个想法，伙计们，这几乎是上帝，因为同样的事情我们正试图做好，我们实际上正试图应用我们实际上在熊猫身上做的任何事情，当你处理缺失的数据时，这非常非常方便。

好吧。

让我们继续下一件事。

现在让我们来填充缺失的值填充缺失的值九阶再次填充缺失的值我将使用 Vf 眼点点填充点好的抱歉 na 点填充好的。

在这个字段中，有两个参数，一个是值，一个是子集。

现在，假设我这样给值，假设我说缺失值，如果我写点 show，那么每当有一个无效值，它就会用缺失值替换。

这里你可以看到空值在这里。

所以，缺失值缺失值缺失值。

假设，如果您真的想只在特定的列中执行这种缺失值处理，那么您基本上也可以像这样编写列名。

这将是我的 Excel 子集。

我也可以给出多个这样的记录。

看，我也可以给多个神像经验因果报应大概年龄，因果报应年龄在召唤入伍，对，当我这样给的时候，那么它这种功能性会发生在两列，对吗？非常简单。

到现在为止，下一步，我们要做的是，我们要取一个特定的列，可能我们要借助该特定列的平均值或该特定列的中值来处理缺失值。

现在，如果我去查看我的 bf 得到了 pi spark，如果我去查看我的点显示值，这是我的整个数据集。

现在，我要做的是，我要取这个特殊的经验列，可能用经验本身的平均值来代替空值。

为了做到这一点，我将使用一个内置函数。

伙计们，如果你们知道输入函数，我们基本上是在 SK 的帮助下使用的，在 PI Spark 中也有，我们有一个不纯的函数。

所以我要把代码复制粘贴到这里，让它变得非常非常简单。

从 pi spark . ml dot feature import in pewter，在这里，我只是要给我的输入列，即年龄经验工资，可能我想申请这里的每一列。

然后我只是说，对于年龄经验工资，我只是要找出这个点格式点 c 输出列。

然后我会保持策略不变，意思是你也可以改变策略，立竿见影比什么都重要。

我将执行这个，这个已经很好地执行了，然后我们将进行适当的拟合和变换。

因此估算出反映 pi 火花点变换的 df。

因此，一旦我执行这个家伙的，你将会看到，我们将创建多个列，下划线作为这个名称。

这里你可以看到 h 下划线被输入。

简而言之，我们在这里尝试了某种均值功能，这基本上意味着空值已经被均值所取代。

在这里，你可以看到这个空值被 28 代替了。

类似地，这个 to null 值被替换为 10 和 5，对不起，是 5。

这就是经验估算栏。

在这里，您会看到，只要有空值，就会被经验列的平均值、年龄列的平均值和薪金列的平均值所取代。

通过这种方式，如果你真的想继续使用中位数，你就能做到。

去把这个平均值改成中间值，然后试着执行它。

给你。

现在你可以看到中间值，这是你的初始空列，抱歉，这是没有值的列。

这是所有的列，基本上都是关于平均值的估算值。

所以，伙计们，今天我们在 pi 火花数据帧的教程。

在这个视频中，我们将讨论滤波器操作。

在数据预处理技术中，滤波操作非常重要。

如果您想基于某种条件或某种布尔条件检索某些记录，我们完全可以借助过滤操作来实现。

现在，伙计们，请确保你们遵守 pi Spark 的播放列表，我会上传越来越多的视频。

记住还有一件事，很多人抱怨说用 Python 上传 SQL。

别担心，我会开始用 Python 上传 SQL。

我非常抱歉，因为一些延迟，因为我正在做某种工作忙于某事。

但我会确保上传所有的视频。

因此，SQL 与 Python 并行也将得到上传。

所以我们继续吧。

首先，让我去做一些细胞。

今天，这个应用程序采用了我们的数据集，一个小数据集，称为 test one dot CSV。

在这里，我有一些数据集，如名称只是经验和工资。

我将用它来展示一些滤波器操作的例子。

最初，无论何时想要使用 PI Spark，都必须确保安装了所有的库。

所以我要用 plus pi spark dot SQL 导入 spark session。

这实际上会帮助我们创建一个火花会话，对吧。

这是我们想用 PI Spark 工作的第一步。

因此，我们将使用 sparks 会话点构建器点应用程序名称，然后我将创建我的应用程序名称作为数据框。

基本上正确地得到我们的创建函数，它实际上会帮助我快速创建一个 spark 会话，我想这是你们每个人都非常熟悉的。

让我们继续，除非尝试读取一个特定的数据集。

所以在这里，我要做的，就是创建一个变量，df 下划线 pi 火花，我要用火花变量点读点 CSV。

这里，我将考虑我的数据集 test one dot CSV。

在这里，我要确保我们选择了这个特定的选项，header 等于 true，in for schema 等于 true，我想这就是我向你们解释的全部内容，如果我写 df.pi，这里会出现一个点，你们可以看到你们的数据集。

好，现在是读取，让我们看看如何得到输出。

这是我的全部输出。

现在，伙计们，正如我向你们展示的，我们将进行过滤操作，我将尝试根据一些条件检索一些记录。

记住，熊猫也有过滤器。

但是在那里你尝试用不同的方式写作。

让我向大家展示如何使用 pi Spark 来执行滤波器操作。

好的，那么过滤操作，让我把它作为降价。

所以看起来很大。

看起来棒极了。

让我再做一些完美的细胞。

现在第一步，我如何做一个过滤操作，假设我想找出少于 20，000 的人的工资。

好吧，小于等于 2 万。

同样，我希望少于或等于 20，000。

对于这一点，有两种方法，第一种方法，我会试着用过滤操作。

所以你有像点过滤器。

在这里，你只需要指定你想要的条件。

假设我写工资小于或等于 20，000。

记住，这个薪水应该和这里的栏名一样，对吗？而当我写 dot show 的时候，你就能够看到这个具体的记录。

你会看到，好的，小于或等于 20，000 是外国人吗，桑尼·保罗在这里表现得很清醒，你会随着经历看到所有这些事情，对吗？现在这是一种方式，可能我只是想捡起来。

在给出这个特定的条件后，我想选择两列。

所以我能做的，就是利用这个。

然后我就可以写点选了。

而在这里，我要指定我的名字，大概我要名字和年龄，名字，逗号，年龄。

所以 dot show，我来做这个。

这就是你可以再做一次的方法。

在这里，你可以看到年龄下划线。

之后你就能得到具体的信息了。

可能我想做一些你实际上能做的小于大于你想做的任何事情的操作。

可能我要放两个不同的条件，那我该怎么放呢？让我们看看。

让我们也看看。

所以我写除 dfπ火花点θ。

这里我要指定我的第一个条件。

假设这是一种方式。

这是使用过滤操作的一种方法。

伙计们，那么我写的这个条件，我也可以这样写，假设我写 dfπspark of salary，假设工资小于或等于 20，000，我也可以这样写，我也能得到同样的输出。

在这里，你可以看到同样的输出。

现在，假设我要写多个条件，我怎么写，很简单，我就拿这个，这是，首先，这是我的一个条件。

所以我就用这个条件。

而且我还可以用一个 And 运算你知道的，所以我会说 AND，or，或者你想要的任何一种运算，大概，我想说 df 下划线 pi 工资很棒，小于等于 2000 20000。

可能我想要一个年薪大于或等于 15，000 的 df pi spark。

所以我能拿到所有的具体记录，好吗？

同样，我会试着把这个放在另一个括号里，确保你这样做，否则，你会得到一个错误。

好吗？非常非常简单，伙计们。

让我们来看看我实际上是怎么写的，大概是这样的，d，f 下划线，pi 点状过滤器，d f 或 pi 点状过滤器，薪水小于或等于 20，000，大于等于 15。

如果我执行，你会看到 15000 到 20000，你会发现你也可以写，或者你会得到所有不同的值。

现在，这是我们的过滤操作，你基本上可以指定，记住，这将非常方便，当你可能检索任何类型的数据集的一些记录时，你可以尝试不同的事情。

这是一种方法，你可以直接提供你的列名，并在内部设置一个条件，这个 PI spark，实际上 pi spark DataFrame 理解它，你就可以得到输出，对吧。

是的，这就是这个视频的全部内容，我希望你喜欢它，我希望你喜欢这个特殊的过滤操作，试着从你的角度去做。

好了，还有一个手术基本待定，我也可以这么写严重大家，我基本可以这么说，好了。

大概，我可以用这个操作，叫做打结操作。

让我们看看这个结的手术将如何进行。

好吧。

基本上，逆条件运算，我们基本上说，所以我会用这个，好的。

还有这个，在这个里面，我可以放一个结条件，就像这样，所以我说这是一个π星火工资 df 小于等于 20，000 的结。

所以任何大于 20，000 的都会在这里给出。

好的，那么逆运算，你可以在单词过滤运算中看到，伙计们，我们将继续π火花系列。

在这个视频中，我们将看到按聚合函数分组。

我已经在 pi Spark 上创建了大约四个教程，这基本上是第五个了。

同样，这是数据帧的一部分，这就是为什么我们实际上应该再次使用 group by 聚合函数来进行某种数据预处理。

让我们从这个特殊的数据集开始。

对于这个特殊的问题，我创建了一个数据集，它有三个特征，比如姓名部门和薪水，你有一些数据，比如崩溃数据科学，薪水，对，像这样的东西。

所以我们在这里简而言之，如果我想基本上理解这个特定的数据集，有一些部门可能是 crash 和其他人教书的地方。

而且基于不同的部门不同，他们拿到的工资也不同。

让我们看看如何通过聚合函数执行不同的分组，以及如何预处理或从特定数据中获取或检索某种结果。

首先，我们要做的是，首先导入 pi Spark SQL 导入 Spark 会话。

像往常一样，我们必须创建一个火花会议。

在这之后，我们要做的是，创建一个火花变量。

因此，我将使用火花会话点生成器点发生。

我想每个人都对此很熟悉。

但是，我还是想给你们看这个，所以让我把它写成聚合点自动创建。

所以现在我实际上已经创建了一个 spark 会话。

好的，这可能需要一些时间。

现在，如果我去检查我的 spark 变量，这是你的全部信息，好的，关于这个特定的 spark 视频，现在让我们继续，尝试读取数据集。

现在我将只写 df 下划线 pi 火花。

然后在这里我会写火花点读点 CSV。

CSV 文件名基本上是 test three dot CSV，记住我也会在 GitHub 中给出这个特殊的 CSV 文件。

然后我会用 header = true 逗号，推断 schema = 2。

这是我的 df 下划线π火花。

现在我将在下一个语句中，写 df 下划线 pi Spark。

点向右显示。

现在，您将能够看到，我实际上能够看到所有数据集。

在这里，我列出了所有这些特定信息的部门和工资。

如果我真的想看到模式或列，比如它所属的所有列，比如数据类型，那么我肯定可以使用 F 下划线 pi 火花点打印模式，对吧。

现在你可以看到姓名是一个字符串，部门是一个字符串，薪水基本上是一个整数。

好的，现在让我们先按操作分组，我们先按操作分组，可能我想按名字分组，可能试着看看平均工资是多少。

假设我们举一个具体的例子。

所以我会写 TF 点下划线 pi 点 group by 假设我想去看看，在这个特定的数据集中，所有这些人当中，谁的工资最高。

所以，我将首先按名称分组，如果我执行这个，你可以看到我们将在某个特定的内存位置得到一个组数据的返回类型。

你应该知道，一组一组的聚合函数是一起工作的。

这基本上意味着首先我们需要按功能应用分组，然后我们需要应用聚合函数。

你真的想检查聚合函数吗，只需按点号和 tab 键。

因此，在这里，您将能够看到许多不同的函数示例，如聚合平均计数最大平均值，更好地，对吗？现在我要做的是，我要用这个点和，因为我真的需要从所有有最高工资的员工中找出最高工资。

在这里，我说 Datsun，如果我执行它，您将能够看到我们得到 sequel 点数据帧，它有名称和工资总额，这非常重要，让我们计算工资总额，因为我真的希望得到工资总额记住，我们不能在字符串上应用 sum。

这就是这里没有给出名字的原因，因为我们已经按名字分组了，有些点将应用于这个特定的薪水。

现在，如果我在这里写点显示，你将能够看到答案，这里是最高工资 35，000，Sonny 有 12，000，Krish 有 19，000，Mahesh 有 7000，所以，如果你在这里看，那么，uncial 基本上存在于这里和大数据中。

所以，总的来说，如果你计算的话，他的工资应该是 35，000，同样，你可以在这里计算我的工资，在这里计算这个，然后你也可以计算 sunny 的工资。

你也可以看到我的散列，这只是一个例子。

所以在这里，我将只写我们已经分组，以找到最高工资。

从这整个观察中，我们可以确定 sudhanshu 的工资最高。

好了，现在让我们先行一步。

领先一步。

现在我们试着按部门分组，找出哪个部门给的薪水最高，好，我们要按给出最高标准的部门分组假设这是我的，这是我的要求，好，可能会有不同类型的要求，我只是想给你们看一些例子。

我只是要复制这个。

我要用这个部门好的。

然后我基本上要说点一些点显示，如果我执行它，让我看到部门是一个错误的列名。

所以我写部门，它是部门。

所以让我来写这个。

现在，如果我在这里看到 IoT 向所有最简单的员工提供大约 115 1000 英镑的工资，因为我们正在做一些大数据提供大约 15，000 英镑的数据科学礼物大约 43，000 英镑我想如果我在这里看到大数据 4000 4000 8000 8013 1000 13，000 15，000 英镑，所以我希望我得到的是，大数据实际上给了我们 15，000 英镑，所以你可以去计算一下

假设如果你想找出平均值，你也能找出平均值，好的，让我把它写在这里。

把这整个复制下来，粘贴到这里，然后读给我听，而不是一些，我会试着写 mean，所以默认情况下，这里的平均工资，你可以看到 IoT 某个地方的某个员工是 7500，因为这个平均值是基于这个部门工作的人数，对吗？

就像这样，你实际上可以发现，现在我还可以检查另一件事，我可以复制它，我可以试着根据部门找出实际工作的员工人数，这样我就可以使用点计数，然后如果我正确地执行它，这是一个方法。

现在，您将看到物联网领域有两个人在从事大数据工作，数据科学领域有四个人，他们是四个人。

所以，现在这里的四加四加八的员工总数基本上是，我可以应用一个直接聚合函数的另一种方式。

现在，看这些都是一些例子，你也可以做不同的分组，让我用 df pi spark，假设我说点聚合，好的，在里面我会给出我的键值对，就像这样假设我说，让我说薪水，我想找出薪水的总和，总的薪水，基本上是给里面的总支出。

所以，你可以看到总支出在 73，000 左右。

所以，我们也可以应用直接聚合函数，否则这也是聚合函数，我们基本上是在你知道应用分组函数之后应用的。

现在，假设这些可能是我想知道的工资，假设我举这个例子，我想知道，得到最高工资的人的最高工资，抱歉。

所以，这里我不写点和，而是写最大点显示。

现在，在这里你可以看到，苏丹显示这里基本上有 20，000 个 10，000 个崩溃得到 10，000 个匹配得到 4 个 4000 对。

所以，所有这些特定的数据都在那里，看，奎师那基本上在这里得到了关于数据科学的 10，000，所以，它基本上选择了它不是选择了两个记录，但至少当它按名称分组时，然后它显示这个特定的数据，那时你将能够看到它，让我们看看我是否也能够看到它。

如果我得分了就分组，写 min。

所以在这里，你可以看到不同记录的最小值，当我在这里分组时，你可以看到那只苏丹鞋，抱歉。

所以答案是最低工资 5000 2000 压榨工最低工资 4000。

对，我们也可以得到特定的信息。

现在让我们看看所有不同类型的操作是什么，它们的平均值也在这里。

所以如果我写一个 VG，它就像只意味着家伙。

所以这基本上是平均工资，同样，你可以检查不同的功能，为什么这些都是基本需要的。

了解一件事，你真的需要做很多数据预处理，很多你基本上做的检索技巧，你可以检查这个，你可以做你喜欢的不同功能，spark emulate 也有一个关于各种例子的惊人的文档。

所以在这里，你可以点击例子。

基本上检查这个特定的文档，你可以看到不同种类的例子，它是如何基本完成的。

但是关于 spark ml，有两种不同的技术。

一个是 RDD 技术，一个是数据框 API。

现在我们要做的是，伙计们，数据框 API 是最新的一个，你知道，它在任何地方都被广泛使用。

因此，我们将重点关注数据框 API。

这就是我们在 PI spark 中非常好地学习数据帧的原因。

因此，我们将尝试通过数据框 API 来学习。

我们将尝试看看我们如何基本上解决机器学习用例的技术。

现在让我们来看一个非常简单的例子。

永远记住，文档是非常令人惊讶的，你可以在这里查看并尝试阅读所有这些东西。

好吧。

所以我们继续吧。

让我们试着看看，我们能做些什么。

在这个特殊的例子中，我只需要一个简单的机器学习问题陈述。

所以让我为你们所有人打开一个特定的数据集，然后可能会尝试这样做。

好吧。

这是我的数据集。

伙计们。

我知道这里没有多少记录。

好的，我有一个数据集，有名字，年龄，经历和薪水。

这只是一个简单的问题陈述，向你们展示 SPARC 实际上有多强大，关于 M 实验室库，只是向你们展示一个演示。

在下一个视频中，我将向大家详细解释回归算法，我们如何基本上实现所有的理论和所有的人都已经上传了，你可以在这里看到，我将在本教程后看到，这基本上是教程六，我将尝试在此之后添加它，然后每当我将在此之前上传线性回归算法时，请确保您观看此匹配灌输。

好吧，我已经上传了这个视频也在同一个播放列表。

所以在这个教程之后 26 St。

yt 说教程 26 因为我也把这个加入了我的机器学习播放列表。

在这之后，你们也能找到，当我们讨论线性回归时，我们如何深入实施，视频也会上传。

所以我们继续吧。

这是我的全部数据。

伙计们，这是我的数据集。

现在我要做的是，根据年龄和经验，我需要预测工资，非常简单的用例，不需要太多的数据预处理，不需要太多的转换，不需要太多的标准化，好了，我要开始讨论这两个独立的功能了。

我会根据这个人的年龄和经验来预测他的薪水。

好吧，这就是我要做的。

这是一个完美的例子，detailee，我会试着向你展示如何基本上一行一行地实现，可能在接下来的视频中，我会讨论线性回归，如果我看到这个特殊的问题，这也是一个线性回归的例子。

好吧，我们从这里开始。

首先，像往常一样，我将创建一个 spark 会话。

所以我将使用从 pi spark 点导入 spark 会话的 SQL。

然后我将使用 spark 会话点构建器点应用程序名称。

在这里，我实际上是在 missing 上创建一个 spark 会话，让我来执行它，我认为这很熟悉，你也很熟悉，然后我在这里要做的是，我们只是读取这个特定的数据集，测试 one dot CSV header 等于 true，并推断 schema 等于 true。

所以当我去看我的训练点显示时，这些都是我的特征，很好，我会给你这个数据集。

还有，不用担心。

现在，从这个特定的数据集，如果我去检查我的打印模式，所以在这里，你将能够看到我得到了这个特定的信息。

这是我的整个打印模式。

在这里，我有姓名、年龄、经历和薪水等特征。

如果我看到训练点列，这是我的训练点列。

现在，请记住 PI Spark 中的家伙，我们使用不同的基金或机制或一种数据预处理，通常我们所做的是，不使用机器学习算法，这是可用的净标量，我们基本上做一个训练测试分割。

然后我们首先，把它分成独立特征和从属特征，对吧，我们用一个 X 和 Y 变量，然后我们做训练测试分割。

通过这样做，在 PI Spark 中，我们只是做一些不同的技术，我们所做的是，是的，我们必须基本上创建一种方法，我可以分组我所有的独立功能。

所以我可能会试着创建一个向量汇编器，我们基本上称它为向量汇编器，看看我实际使用的类，向量汇编器会确保我把我所有的特征组合在一起，就像这样，以年龄和经验的形式，假设在这里，我的两个主要特征是年龄和经验，这是我独立的特征。

因此，它将像这样分组，对于每个记录，它将像这样分组，好的，对于每个报告，它将像这样分组，然后我要做的是，我将把这个组视为一个不同的功能。

所以这基本上是我的新功能，对吧。

而且记住，这个新功能是我的独立功能。

所以我的独立特性在一组 H 逗号体验中会是这个样子，会被当作新特性。

而这正是我的独立特色。

所以我必须以这种特殊的方式分组。

因此，为了对其进行分组，我们在 PI Spark 中使用了一种叫做矢量汇编器的东西。

所以在这个矢量汇编器中基本上存在 pi spark.ml 的点功能，我们用这个矢量汇编器，我们用两个东西。

一个是输入列，我们基本上是用它来对所有列进行分组。

所以有两栏，一栏是年龄和经历，对，我们不用取名字，因为名字是固定的，它是一个字符串。

是的，如果有分类功能，我们做我们需要做的事情，我们会将其转换为一些数字表示，当我做一些深入的实现时，我会向您展示即将到来的线性回归、逻辑回归等视频。

但是在这里，你可以看到，我要以列表的形式输入列 age come experience。

然后，我将尝试对其进行分组，并创建一个新列，在这里称为独立功能，对，这就是我实际正在做的事情。

如果我去执行这个向量组装器，这里，我有我的特征组装器，然后我做点变换，我对我的训练数据做点变换。

当我这样做时，这基本上是我的训练数据，当我在这里输出点显示时，你可以看到我有所有的功能，一个新的功能已经创建，它被称为独立功能。

好了，我们实际上已经创建了一个独立的功能。

你可以在这里看到，年龄和经验，年龄和经验，年龄和经验，所以这是我实际上得到的分组行，简而言之，我做的是，我把这两列组合起来，使之成为一个独立的特征，好吗？现在，这将是我的输入功能。

好吧。

这将是我的输出特征，我们将尝试训练模型。

好了，现在在这里，如果我去看看输出点列，我有姓名，年龄经验，工资独立的特征。

现在我要做的是，取我真正感兴趣的所有数据集。

所以除此之外，我只会对这两个数据感兴趣。

单独的独立特征和薪水薪水将是我的输出特征，y 变量，对，这将是我的输入特征。

所以我要做的是，我要选择输出点选择独立功能和工资。

我将把它放入最终的下划线数据中。

这就是我正在做的事情。

如果我现在去看我的点显示，你将能看到整个事情。

这是我的独立特征，这些是我的从属特征。

现在第一步，我们做的是训练测试分割，就像我们在标量中做的那样。

因此，为了进行训练测试分割，我在最终数据中使用了一个函数，称为随机分割。

记住，伙计们，当我在做一个更大的项目时，我会尝试通过实现它来一行一行地向你们解释。

现在，因为这只是一个介绍会议，我真的想向你解释事情实际上是如何做的。

所以这基本上是我的火车测试分裂。

所以在这里，让我写下评论，火车测试分裂。

我将使用线性回归，就像我们如何把一个教室变成一个缩放器。

类似地，通过使用 pi spark.ml，点回归，导入线性回归，然后我随机抽取 75%到 25%。

这基本上意味着我的训练数据集将有 75%的数据，我的测试数据集将有 25%的数据，对，然后在这之后，我将使用一个代理线性回归，在这你有两个重要的变量，我们需要得到一个是特征列，有多少个特征列完全存在于这个独立的特征中。

所以我把它放在这里。

类似地，在标签列中，这是我必须给出的第二个特性，这是我的输出特性。

所以在我提供这两个东西，并对训练数据进行拟合之后，我将能够找出我的系数，这些是我的系数。

这些是我截获的。

在这里，我现在可以评估和查看我的输出。

因此，通过使用 evaluate 函数，我们将能够看到输出，其中将有一个预测变量，它将有输出。

好的，这是我的预测。

这是我的工资，真正的价值。

这是我的另一件事。

现在，如果我真的想找出其他重要的器件参数或指标，让我们按 tab 键，您将能够看到平均绝对误差。

pred 下划线结果点均方误差，假设如果我看到这个特定的单词值，你将能够理解模型实际上是如何执行的。

这只是一个非常简单的例子，伙计们。

不要担心，当我们从线性回归开始时，我可能会在接下来的视频中深入解释。

现在记住，下一个视频是关于深度实现中的线性回归实现，对吗？帮帮忙，你知道到底什么是数据砖块平台。

这是一个惊人的平台，你可以使用 PI Spark，所有你可以用 Apache Spark 工作。

这个特定平台更令人惊奇的一点是，它们还为您提供了集群实例。

因此，假设您有大量数据集，可能您想要分布并行处理，或者可能想要分布在多个集群中，您肯定可以借助数据块来完成。

现在，如果我真的想使用这个特定的平台，有两种方式，一种是社区版本，一种是付费版本，就像 Azure 或 AWS cloud，你可以在后端使用，数据砖也可以帮助你实现 ml 流，好的，这个 ml 流是关于 CI CD 管道的。

所以你也可以做这些实验，总之，这是一个很棒的平台。

我将在我的 youtube 频道中重点关注的是，我将尝试用社区版本向你们展示这两个版本。

在接下来的视频中，我们将尝试使用 AWS 和 Azure 来执行。

当我们使用 AWS 和 Azure 时，我们将尝试做的是，每当我们创建实例时，多个实例，你知道，将尝试在这个特定的云平台中创建，也将尝试从 S3 bucket(AWS 中的存储单元)中提取数据，并尝试向你展示我们如何处理巨大的数据集，当我们继续前进时，正确处理所有这些事情。

现在，我们来了解一下什么是数据砖，它是一个开放、统一的数据分析平台，用于数据工程、数据科学和机器学习分析。

请记住为什么数据砖实际上帮助我们执行数据工程，当我说数据工程时，可能与大数据一起工作，它也帮助我们执行一些机器学习算法。

大概任何一种数据科学的问题陈述都能够做到。

还有 Rob，我们假设三种平台云平台一个是 AWS，微软 Azure，Google Cloud。

现在，如果你真的想从这个开始，我们将从社区版本开始。

你只需进入这个特定的网址，键入 try data bricks，然后输入你所有的详细信息就可以免费注册了。

现在一旦你注册了，当你想免费开始的时候，你会有两个选择。

在右边你会看到社区版本，你真的想免费使用它。

在左侧，您将有一个选项，我会告诉您，您需要使用这三个云平台。

你也可以选择这个。

所以现在，我会试着给你们展示一个社区版本，非常简单，非常非常容易。

所以还是去社区版吧。

这就是社区版的真实样子。

如果你真的想进入云版本，你可以点击升级。

好的，点击升级。

这是社区版的网址，明天你注册社区版的时候就能得到这个网址。

所以你认为你可能想使用云，你现在只需要点击这个升级。

现在，在这里，您将能够看到三个内容:浏览快速入门教程、重要的浏览数据、创建空白笔记本以及许多其他内容。

在这里，你可以在社区版本 1 中完成什么样的任务，你可以创建一个新的笔记本，你可以创建一个表，创建一个集群，创建新的 ml 流实验，我希望我真的向你展示了 ml 流体验，我们也可以通过在后端结合到一个数据库来创建这个 ml 流实验，好的，然后我们可以导入库，阅读文档，做很多任务。

首先，我们需要做的是，我可能会创建一个群集。

为了创建群集，我将单击此处的“create a cluster ”,您基本上可以写下任何群集名称。

假设我说阿帕奇，或者我就说，pi Spark 集群。

假设这是我想要创建的集群。

好的，默认情况下，在这里，你可以看到 8.2 的缩放器，这个 spark 3.1 点被选中。

因此，我们将使用 spark 3.1 point one，如果您还记得，在我的本地，我实际上只安装了这个特定的版本，默认情况下，您将能够看到他们将为您提供一个 15 GB 内存的实例，以及一些更多的配置。

如果你真的想升级你的配置，你可以点击这里。

请记住，在免费版本中，您将能够在一个实例中工作，除非并且直到它没有空闲两个小时，否则它将被断开。

在这里，您可以看到一个驱动程序 15.3、GB 内存、两个内核和一个 dBu。

好的，所有这些都在那里，您也可以理解视图是什么，DB 只不过是一个数据、砖块单元。

如果你想点击这里，你将能够了解什么是正确的看法。

你将能够选择一个云，并基本上与完美的工作。

直到这里，一切都很好。

让我们开始，让我们创建集群。

现在，您将看到群集基本上已经创建完毕。

这里还有很多选项，比如笔记本库、事件日志、spark UI 驱动程序日志等等。

它不像你刚刚有，你将能够在这里与 Python 一起工作。

这里你有很多选择。

好吧。

假设我点击“libraries ”,如果我点击“install new”这里，您将有一个上传库的选项，您也可以从 Maven 的 pi pi 安装库，它将基本上与 Java 一起使用，然后您会有不同的工作空间。

所以在这里，我要做的是，假设你选择 by by，假设你想安装一些像 TensorFlow 这样的库，或者可能想安装 Kara 的，你基本上可以这样写，可能我想要一个刻度，你知道，所以我可以用逗号分隔并开始安装它们。

好吧。

但是默认情况下，我知道我要用 PI Spark，所以我不会安装任何库。

所以让我们看看这大概需要多长时间。

这只是在这里执行。

我们回我家吧。

除了今年，你还可以上传数据集，这些数据会给你一个环境，比如你如何在循环中存储数据。

在创建群集之前，好的，现在群集已创建，您可以看到 pi spark，它处于运行状态。

现在。

请记住，该集群只有一个实例，如果您想要创建多个集群，我们必须使用云平台，这将是收费的。

好吗？在这里，我将单击导出数据。

现在，你们可以上传数据，也可以从 s3 存储桶上传，或者从 s3 存储桶上传。

这些都是我想给你看的东西。

然后你还有 dbfs，你知道，和 DB FF，你将基本上存储在这种特殊的格式。

还有其他数据源，如 Amazon redshift、Amazon kinases，Amazon kinases 基本上用于实时流数据。

好吧。

然后你有 Cassandra，Cassandra 也是一个没有 SQL 数据库和 JDBC lastic search 所以不同不同的数据集，数据源也将有也尝试看看与一组合作伙伴的集成。

因此，它们也像数据湖中的实时捕获以及其他许多东西。

所以你一定可以看看这个。

现在我要做的是，单击这里，尝试上传我们的数据。

让我看看。

让我上传一组数据。

我就去我的 PI spark 文件夹。

这是我的π火花。

因此，我只是要上传测试数据集可能没问题。

上传这个测试。

现在，您可以看到数据集已经上传。

现在说的是用 UI 创建表在 Node back 笔记本中创建表。

假设我去点击这个，你知道。

在这里你可以看到这是代码，这是在 UI 中创建一个表格的全部代码。

但是我真正想做的是，我不想创建一个表，我只是尝试执行一些我们已经学过的 PI spark 代码。

现在，好的，我要做的是，把这个拿掉，我不想要它，我会把它拿掉，好吗？

好吧，让我看看数据。

现在来读数据集。

在这里，您将能够看到我的数据集路径基本上是这样的，它是一个 CSV 文件。

在全模式标题模式中，所有这些都在那里。

让我把这个也拿掉。

所以让我开始阅读数据集。

所以默认情况下 spark 已经上传了。

所以我会写 spark 点 spark 点 read 点 CSV，我希望它能工作，第一次，记住，这是我的文件位置。

文件位置。

好的，再见下划线提升。

然后我还会用另外两个选项，一个是 header 等于 true，然后我推断 schema 等于 true。

一旦我执行了这个，现在你会自动看到。

第一次执行时，它会说启动、启动并运行，我们将启动群集并运行它，我将单击它无法创建拒绝请求，因为节点总数将超过限制 1，这是为什么。

让我们看看我们的集群，我们只有一个集群。

好的，这里有一些例子。

让我去掉其中一个。

好吧，让我来执行一下。

好吧，我去那边。

空间，我删了吧。

好吧。

完美。

现在，我试着读一下这个。

让我们看看。

同样，它说无法创建集群拒绝请求被拒绝，因为节点总数将超过 1 的限制，它不允许我们执行一个以上的文件我猜。

正因为如此，我才重新装上。

让我们现在看看。

现在它已经被执行了。

因此，它不允许我运行，我只是删除了一个文件，然后重新加载了一个文件。

所以现在你可以看到它正在运行。

好的，你也可以按下 Shift Tab 键来查看一些提示，就像我们在 Jupyter Notebook 中做的一样。

现在你可以看到我的文件运行得非常好。

它显示了这个 df，它显示了，这是一个 PI 火花点 SQL 点数据帧原始数据。

现在，让我来执行其他的事情。

现在，假设我想要 df 点读取，看到我只是使用标签功能打印模式，如果我去看看这里，你将能够找到所有正确的值。

简而言之，这基本上是在我的群集实例中运行的，我将能够上传任何巨大的数据集，可能是 50 gb 的数据集，也来自 s3 存储区，在接下来的视频中，我将尝试向您展示我们如何从 s3 存储区做到这一点。

但是在接下来的时间里，我将向你们展示的是，尝试通过数据砖来运行所有这类问题陈述，这样你们就能够很好地学习了。

现在，让我再去做一件事。

所以这是我的 df 点显示。

好的，这是我所有的数据。

因此，我可能只想选择一些列，我实际上可以写 DF 点选择，这里我只想说工资点显示我只是选择工资点显示在这里，您将能够看到您想做的一切，您将能够做到这一点，请记住，在这里您将能够找到大约 15 gb，您肯定可以执行任何类型的事情。

这里也有同样的选项，就像我们在 Jupyter 笔记本中所知道的那样，每个选项都是您能够在 Jupyter 笔记本中找到所有这些特定选项，对吧。

因此，这基本上是在 15.25 gb、两个内核中运行，好的，在那个特定的群集中，您有两个内核，然后您有 spark 3.1 点一个 Spark 2.12，您将能够看到所有这些特定信息。

所以我想要的家伙，请尝试为你做一个特定的环境，然后尝试开始它，尝试做好一切准备。

在接下来的视频中，我们将尝试了解如何实现问题陈述，如何实现不同的算法。

上节课我已经给你们介绍过数据砖了。

我希望你已经做了你的帐户，我希望你已经开始使用它。

如果不知道怎么做账，请观看罗技教程七，整个播放列表链接会在描述中给出。

这是我的 databricks 社区账户。

请记住，在社区版本中，我们只能创建一个集群。

我也会给你看升级版的，可能在将来，我会买的。

我将尝试向您展示如何创建多个集群，无限的集群。

但为此，你还需要使用一些像 AWS 或 Azure 这样的云。

现在，首先我要用什么数据集。

这是我将要使用的全部数据集，这个数据集叫做美国小费数据集。

这基本上意味着实际去餐馆的人，他们实际上根据总账单给了多少小费，或者我也可以根据所有这些特定的参数来解决这个特定的问题，这个人将要支付的总账单大概是多少？好，这就是我要解决的问题陈述。

现在在这里，你可以看到这是一个多元线性回归问题陈述。

在这里，你有很多很多特点，对不对？所以我们继续吧。

首先，我要做的是，单击浏览，然后上传这个特定的数据集。

现在，为了上传这个特定的数据集，我的路径中有这个特定的数据集。

所以我可能也会给你们这个特殊的数据集，所以不用担心。

哦，让我快点，等一下，让我把数据集上传到这里。

好吧。

通过 Spark，好的，所以在这里，你可以看到这是我的数据集，我实际上是在上传技巧。

所以让我打开它吧。

现在，在这里，您将能够看到您的 tips 数据集将上传到 DB Fs 目录中。

所以在这里，你会有类似文件存储斜杠表的东西。

好吧。

现在，我们可以点击这个 dvfs。

在这里，您可以看到文件存储，也可以单击表格。

这里你有步骤点 CSV，我也上传了我以前的视频溶解数据集，可能我只是用这个，好吧，但在这里，我只是侧重于提示点 CSV。

现在我要做的是，让我们开始第一步。

记住，数据砖块的第一步是我们需要创建集群，好吗？并创建一个集群。

现在，默认情况下，在社区版本中，数据块实际上帮助您创建一个集群，只是一个集群。

但是如果你使用付费版本，升级版本，如果你有 AWS 云的访问，它实际上会帮助你创建多个集群。

我将单击该群集，让我创建一个新的群集。

我会说这是我的线性回归聚类。

然后我将使用这个运行时 8.2 标量，这是那里。

我们只需单击该群集，其余所有内容在该群集中几乎相同。

在这种情况下，您将获得 15 GB 内存和所有其他信息，您可以查看。

您也可以获得两个内核和一个 I dB。

好的，实际上我在之前的课程中已经讨论过了，我将单击“cluster”。

这需要一些时间。

记住，伙计们，如果你真的想使用任何类型的库，只需点击这里并安装那些他们想要的库。

比如假设你想用 Seabourn，你想用 kiraz，你想用 TensorFlow。

所以在这里你基本上可以把它和版本一起输入，你就可以安装它了。

但是现在，我不要求任何库使用 PI spark，这是我的主要目标。

伙计们，点击这里的集群。

在这里，你可以看到大概一分钟后，这个特殊的集群实际上被创建了。

好吧。

现在，再次回到家里，你可以做什么，你可以创建一个空白笔记本，我已经创建了一个笔记本，这样我就有了基本的代码。

所以我要打开它，让我们开始这个特殊的过程。

现在首先，我有一个叫做文件位置的东西，我知道我的文件位置基本上是 tips dot CSV，文件类型是 CSV。

然后我就用 spark 点 read 点 CSV 文件位置头等于 true info，schema 等于 2。

让我写 df 点显示，这实际上会帮助我检查整个数据集。

好，我就在你面前执行它。

让我们一行一行地写，我会试着写下所有的代码，这对你理解肯定有帮助。

所以请确保你也和我一起打字，以便更好地理解代码。

好，现在我要执行这个。

在这里，您将能够看到我的，我的群集将开始运行。

好吧。

然后，您可以看到正在等待运行命令，可能我们将能够看到它，只需缩小一点，以便您能够正确地看到它。

再次，伙计们，第一次，如果你开始这个特殊的集群，这将需要时间。

好吧，它正在运行的火花作业。

现在你可以看到我的数据集了。

那是我的 tips 数据集，上传在这个特定的文件位置。

这是我的全部数据，账单总额，小费，性别，摩卡日期，时间长度，完美。

现在我们进行下一步。

我要做的，就是写 df 点打印模式。

所以我可以哦，所以，你捅，你知道，它将能够加载整个事情。

所以现在你可以看到这是我的全部账单，小费，性吸烟者一天的时间。

这是你所有的特征，比如双性感的字符串吸烟者是字符串，日期字符串时间字符串整数。

现在请记住，您可能会想 Krish，为什么我实际上在 databricks 中这样做，只是为了让您了解这将如何在集群中运行。

现在我只有一个群集，这基本上意味着这个特定群集中的最大 ram 大约为 15 gb。

但是，请理解，如果您正在处理 100 GB 的数据，会发生什么情况，这种处理会分散到多个群集中，对吧。

这样，您就能够在即将到来的“正确的事情”中使用大数据。

我认为这是对的。

现在让我们来试着理解这里，这是我的独立特征，我的独立特征是我的提示特征性吸烟者一天的时间和大小。

和我的从属功能，基本上总账单。

因此，基于所有这些特定的功能，我需要创建一个线性回归算法，这将能够预测总账单。

让我们继续，现在在这里，我要写 df 点列。

如果我想检查我的列，这是我的列。

所以我可以看到这是我确切的列，我实际上有这么多列。

现在，关于这个特别的功能，伙计们，你们有像性别，吸烟者，日期，时间这样的栏目，对吗？这些都是分类特征，对吗？可能，你知道，这类特征需要转换成一些数值，然后只有我的机器学习算法才能理解它。

所以让我们看看如何处理类别特征。

所以在这里，我只打算写一个评论。

好的，处理分类特征，对。

现在，我将尝试向您展示如何处理这种类别特性。

现在，PI Spark 中的一种方法，显然，我们知道，在正常的 SK 学习中，我们尝试使用一种热编码，我们尝试使用顺序编码，我们尝试使用不同类型的编码。

类似地，我们也可以在 pi Spark 的帮助下使用相同的编码。

对于这个特殊的过程，我们有一个叫做字符串索引器的东西。

所以我要说的是，从 pi spark radar，从 pi spark.ml 点特性，好的，我要导入一个叫做字符串索引器的东西。

所以，我将使用字符串索引器，字符串索引器实际上会帮助我们，你知道，基本上将我们的字符串类别特征转换为一些数字特征，数字特征基本上是序数编码。

比如假设我有性别比如男性或者女性，它会显示为 0 和 1。

在这里，你会看到 I，所以这里的大部分类别都是顺序编码。

现在，你可能会想，一个热门的编码，在接下来的视频中，我会用不同的机器学习算法向你展示什么过程？我之所以这样做是因为一次学一件事更好，对吗？我会试着展示所有这些例子，现在让我们继续，试着看看我们如何转换这个类别的特征，比如性别，吸烟者的日期和时间，可能时间也是类别的特征，看这里。

如果我在这里看到所有的特征，让我做一件事。

让我只写 df 点显示。

这就是我的全部特征。

很快，如果我去看看这是时间也是类别特征。

那么很快，让我们继续，试着看看我们如何使用它，让我也删除它，或者让我再写一遍。

所以我实际上，你知道，导入了这个叫做字符串索引器的库。

现在我要做的是，让我这样写我们的索引器对象，我要写字符串索引器。

在这里，首先，我真的需要提供我们所有的类别功能。

现在记住，在这个字符串索引器中，如果我按下 Shift Tab，可能在这里，这里，你会看到我必须给出输入列。

让我接触这里，我必须给出输入列，我必须给出输出列。

我还可以选择将输入列作为多列提供，将输出列作为多列提供。

让我两个都试试，在这里，首先，让我试试输入列。

因此，在输入列中，我将提供我的第一个值。

现在假设我真的想把性别列转换成我的类别特征。

所以我在这里写输出列。

好吧。

在这里，我会说，性别下划线索引。

现在，我们实际上正在做的是，在这里，我实际上给出了我的性别列，这个性别列将在字符串索引器的帮助下被转换成序数编码。

好的，现在在下一步，我要做的，就是写 df，好的，可能我会用 df。

或者我可以做什么，我可以，我可以创建另一个，可能我可以创建另一个数据框。

所以我会写 df 下划线可能是 c，因为我不想改变 DF，再次运行那个特定的代码。

现在，我会说索引器点拟合，好的，所以我肯定可以用拟合。

然后我可以使用 transform。

这里也是一样，只有男生才适合下划线转换。

这里我也要用 df。

好吧。

然后，如果我去看看 df.df 下划线 r 点显示，在这里，现在你可以看到，性别列，另一个性别下划线索引列将被创建，它将在这个特定的列中有顺序编码的值。

所以我们去看看吧。

所以一旦我执行它，完美，我认为它运行正常，这将需要时间。

现在在这里，你可以看到我多了一列，叫做性别下划线索引，女性值为 1，男性值为 0，对吗？所以我们处理了这个专栏。

我们已经基本上把这个字符特征转换成了序数编码。

现在，我们仍然有许多特色。

所以我要做的，就是再次使用这个索引器。

可能我会写在这里，多列，我会指定。

所以第一栏，我已经改了。

所以我要把这个换成别的，性而不是会变成吸烟者的性。

好吧，吸烟者。

但是我给你们看了，现在我要写输入列，而不是写输入列。

在这个多列中，当我给的时候，这是吸烟者，那么我还有一个特征。

如果我看到日、日和时间，日和时间就多了两个特征。

所以我就在这里画下划线。

伙计们，现在我已经写了《烟鬼日日夜夜》，同样，我将在这里再写三篇专栏。

所以第一列应该是，因为我要创建序数编码。

我可能会在这里创建一个新的专栏。

所以这将是我的吸烟者下划线索引。

好吧，我把这里的牙套合上。

我的第二个特性基本上是去下划线索引，对吗？或者，我的第三个特点可能是我们的时间下划线索引。

在这里，我将再创建三个特征。

然后我给出了下标点拟合或 df 下划线 r。

好的，因为现在我有了新的数据框，然后我要说 df 下划线点显示。

现在，一旦我执行它，伙计们，我希望不要给我们一个错误，好吧，这是说，无效的参数值给定的参数输出列不能转换上次恢复，所以我必须使它作为输出列。

好吧，这就是问题所在。

对吗？所以现在你可以看到它执行得非常好。

现在，这里有所有可用的功能六个下划线索引 smoken 下划线索引，de 下划线索引和 time 下划线索引，您可以在这里看到序号编码，如 012，我们现在已经将所有字符串值转换成此功能中可用的所有类别值，转换成数值。

现在，我的模型肯定能够理解好的，现在我们已经完成了这个，现在，让我们继续下一步我们基本上要做什么，我们现在要讨论的其他步骤非常简单，因为我们已经创建了这个特定的数据集。

现在我们要做的是有一个叫做矢量汇编器的东西。

现在，请记住 PI Spark 中的各位，你们知道，每当我们拥有所有这些特定的功能时，我们需要将所有独立的功能组合在一起，并将依赖的功能分开，好的，所以各位，我们将从 pi spark.ml 点功能开始编写，我将导入一个称为矢量汇编器的东西。

对吗？我将使用这个向量汇编器，它实际上可以帮助我们将独立的功能组合在一起，将依赖的功能分开，让我继续编写向量汇编器，然后我将初始化它。我必须提供的第一个参数基本上是我的输入列，这是我的输入列，我有哪些输入列？让我看看，在这之前，让我快速做一件事，让我做一个细胞。

向上创建一个单元格，让我把它去掉，可能让我写，你知道，df 下划线 r 点列。

好的，我们有多少列。

所以我有关于这些列的所有信息。

因此，我的输入栏在这里，我肯定要提供的第一件事是我的提示栏。

因为 tip 是必需的，tip 是第一个独立的特征，它是一个数字特征，然后我有类似六个下划线的索引，好的，所以我要把它复制粘贴到这里。

这是我的另一个输入功能。

记住伙计们，我们真的需要遵守命令。

所以现在我的第三个特点基本上就是烟指数。

在这之前，我也可以指定大小。

所以我会指定尺寸，六号吸烟指数。

好的，然后我可能会创建一个索引。

好，他们索引，逗号，我就用时间索引。

好，这是我们的，这些都是我的独立特征。

关于这一点，现在记住，这将被组合在一起。

我还必须说，如果这是分组在一起，让我们创建一个新的功能，并解开和命名这整个组。

好的，在这里，我会说，输出列等于，这里我会指定这是我的独立特征。

所以我要把这整个东西命名为我的独立特性，非常简单。

现在，让我再做一件事，让我创建一个名为“特性汇编器”的变量，这样我们就能够转换这个值。

好的，所以特征汇编器等同于向量汇编器，这里我必须提供我的输入列和输出列，非常简单，非常容易。

现在我要做的下一步是右输出等于我要说点转换，因为我真的需要转换，这需要从我的 df 下划线艺术转换。

让我来执行它。

现在。

现在，在这里，它已经被执行了，你可以看到整个输出，所有这些东西都被创建了，这些是我的独立特征。

现在，在独立中，我们为什么需要创建这种独立功能，这是 PI Spark 中给出的规范，请始终记住，我们需要创建一组功能，可能还有一个列表，所有这些独立功能将一起完成。

现在，如果我去看看我的输出点显示，这里，现在你将能够看到，我将能够看到一个更多的功能，它有这个，或者让我只写输出点选择，因为可能所有的功能已经分组在一起。

很难在一个屏幕上看到所有功能，我将只显示这些独立的功能，然后单击点显示。

现在，一旦我这样做了，在这里，你将能够看到所有这些特殊的功能。

请记住，这需要以相同的顺序显示。

第一个特征是提示，然后大小六，下划线索引，吸烟者下划线索引，de 下划线索引，时间下划线索引。

所以这些是我的独立特征。

现在我只有一个特点。

在这里，您将能够看到它有一个所有功能的列表，就像这样。

这是创造的第一个重要步骤。

好了，现在我们去下一个。

现在我知道我的输出是什么了。

现在我要做的是，在整个输出中，如果我去查看我的输出，输出点显示在这里，您将能够看到所有功能都在输出点显示中可用。

在这里，我们将能够看到所有可用的功能。

现在你知道了，哪个是输出特性，对吧？所以这是我的依赖特征。

而这种独立的特征就是我的独立特征。

我现在要做的是选择输出，或者说这基本上是我的最终数据。

我将选取两列，即输出点选择。

在这里面，我将给出我的两个特征，其中一个是独立特征。

好吧，独立乐队特色，我希望名字是正确的。

否则，我就再确认一次。

让我点击这里。

独立的功能，一个是总下划线法案。

完美，逗号，总下划线法案。

好吧。

现在，如果我只是去执行这个，现在我只是从中获得一些特性。

现在，如果我去找出最终数据点显示，现在我将能够看到两个重要的功能，即独立功能和总账单。

记住，这都是我的独立特色。

这是我的从属特征。

这是非常简单的直到这里。

如果完成了，伙计们，下一步基本上就是我要复制并粘贴一些代码，我要应用线性回归。

所以首先从 pi spark.ml 点回归，我要导入线性回归。

然后，我将获得全部最终数据，然后随机分成 75%和 25%。

好的，然后在我的线性回归中，我将提供我的独立特征作为我的特征列。

这是线性回归中给出的两个参数，一个是特征列，这里我将提供独立的特征。

第二个基本上是总帐单，这是我的从属功能。

现在我将对训练数据进行拟合。

所以一旦我这么做了，我的回归模型就会被创建。

这可能需要时间。

现在，您可以在这里看到所有的信息，关于培训和测试的惊人信息。

好的，记住伙计们，不管你们组了什么样的独立电影，这都是 UDT 的形式。

好了，你可以看到 UDP 的完整形式，这不是一个大问题，我想，好了，现在我有我的回归。

所以我要做的是，我只说回归量点系数，因为这是一个多元线性回归，所以我只使用常规的点系数，这些都是我的参数不同的系数，因为我有大约六个参数。

这是所有六个不同的系数。

请记住，在线性回归中，您将拥有基于要素数量的系数。

你也将拥有互联系统。

所以这里我要截取。

好，这基本上是我的截距，也就是点 923。

好的，现在你有了两个信息，现在是我们尝试评估，评估测试数据的时候了。

所以在这里，我只说测试。

这基本上是我的预测，对吗？所以在这里，让我把它写成这样。

所以预测，好吧。

预测。

我接下来要做的，就是写 pred 下划线的结果。

结果等于这个。

这基本上就是我的结果了，好吗？或者测试未定义为什么测试未定义，因为应该有测试数据。

我真的很抱歉，好吧。

但是没关系，你可以得到很小很小的误差。

现在，如果我真的想看到我的价格结果，只要去看看红点预测，他们会像预测点显示，好的。

如果你这样写，在这里，你将能够得到整个预测。

好吧。

所以记住，在这个预测中，这是你的独立特征，这是你的总账单，这是你的实际价值。

这是一个预测值，实际值预测值，实际值和预测值在这里，你可以实际比较它有多好，你知道，只要看到你的总账单和预测值，非常好，非常惊人，你可以看到数据，我只是要写我的最终比较。

好了，最后的对比。

完美。

我很擅长这个，你看得出来。

现在让我们看看其他一些信息，比如我们基本上可以查看哪些信息。

从这里，我们可以我们有很多东西去可能你想检查一下 R 广场。

所以你可以写，基本上你可以写一个回归，如果我按 tab，这个系数截距，那么你就输了，然后还有一个 R 的平方。

如果我去执行这个，这基本上是我的 r 的平方。

或者让我写下来。

我认为，预测预测。

我不这么认为，R 平方是让我们看看是否能看到 R 平方值的地方。

请稍等，我正在查看文档页面。

好吧，哦，对不起，我不需要用回归器。

所以这里我将使用预测点结果。

让我计算一下 R 平方。

这是我的 r 的平方。

同样，你也可以检查预测结果点平均绝对误差。

所以你有平均绝对误差。

你也有预测下划线结果点平均，平方。

所以这三个值，你绝对可以查一下。

这是你的平均绝对误差，这是你的均方误差。

因此，这些是我绝对可以拥有的绩效指标。

无论何时，无论何时，当你们面临任何类型的问题时，一定要检查 Apache Spark em lib 文档中的文档。

用这种方法，你肯定可以完成整个问题陈述。

现在我给你们一个任务，试着用谷歌搜索一下，看看如何保存这个特殊的文件，可能是 pickle 格式，也可能是临时的 pickle 文件。

你知道，这非常非常简单，你只需要使用回归点保存，但是试着看一下，试着看看如何保存这个特殊的 pickle 文件。

这都是关于这个特别的视频。

我希望你现在喜欢这个，只是试着解决它任何其他问题的陈述。

试着这样做。

在接下来的视频中。

我还会试着展示你如何做一个热编码，也许你也能学会。

所以我希望你喜欢这个视频。

请订阅该频道，如果你没有订阅，请确保有一个美好的一天。

谢谢你。拜拜。


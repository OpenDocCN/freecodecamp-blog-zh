# 了解面向边缘设备的 TensorFlow Lite

> 原文：<https://www.freecodecamp.org/news/learn-tensorflow-lite-for-edge-devices/>

TensorFlow Lite 是一个开源的深度学习框架，可以在小型设备上使用。

我们刚刚在 freeCodeCamp.org YouTube 频道上发布了一个 TensorFlow Lite 课程。

巴维什·巴特创建了这门课程。Bhavesh 在自己的频道中创建了很多课程，是一个很棒的老师。

TensorFlow Lite 由谷歌开发，用于在移动、物联网(物联网)和嵌入式设备上训练机器学习模型。

当您使用 TensorFlow Lite 时，机器学习全部发生在设备内部。这可以避免与服务器来回发送数据。

以下是本课程涵盖的主题:

*   为什么我们需要 TensorFlow Lite？
*   什么是边缘计算？
*   为什么边缘计算越来越受欢迎？
*   在边缘设备上部署模型的挑战
*   什么是 TensorFlow Lite 或者 TFLite？
*   TensorFlow Lite 工作流
*   创建张量流或 Keras 模型
*   将张量流或 Keras 模型转换为 TFLite
*   验证 TFLite 模型性能
*   什么是量子化？
*   进一步压缩 TFLite 模型
*   进一步压缩 TFLite 模型
*   验证压缩程度最高的 TFLite 模型性能

观看以下全部课程或在 freeCodeCamp.org YouTube 频道观看[(1 小时观看)。](https://youtu.be/OJnaBhCixng)

[https://www.youtube.com/embed/OJnaBhCixng?feature=oembed](https://www.youtube.com/embed/OJnaBhCixng?feature=oembed)

## 副本

(自动生成)

TensorFlow light 可以让你在小型设备上进行机器学习。

bhavesh 是一位经验丰富的讲师，他将在本课程中教授你所有关于 TensorFlow light 的知识。

大家好。

在本教程中，您将学习 TensorFlow light 的基础知识，以及 TensorFlow light 如何帮助您创建可部署在边缘设备上的真正高效的模型。

因此，不浪费任何时间，让我们开始教程。

让我们以一个小故事开始今天关于飞行的讨论。

我有一个朋友，他的名字叫约翰。

约翰真的喜欢去不同的地方旅游。

他最喜欢的应用之一是谷歌镜头。

每当他访问一个新的国家，他都会拿起手机给面前的纪念碑拍照。

谷歌会告诉他这是哪座纪念碑。

所以现在，仅仅是被这个工具的魅力所吸引，约翰继续前进，创造了他自己的神经网络来探测地标。

他经历了一个收集数据、标记数据、清理数据的严格过程。

最后，他创建了一个机器学习模型，可以告诉约翰，这是哪个纪念碑。

所以一切看起来都很好，他也能够达到非常高的准确率。

现在，他面临的唯一挑战是，他应该将创建的模型部署到哪里。

所以技术上来说，他有两个选择。

他首先探索的选项是云计算。

他把他的火车模型放在云上。

他公开了自己创建的 API。

本质上，他创建了一个 Android 应用程序，它可以查询 API，并在他传入图像后获取结果。

现在，他在创建解决方案时发现的主要挑战之一是网络延迟。

所以现在手机拍摄的图像一般在 3 到 10 Mb 之间。

因此，在整个预测过程中，传输如此巨大的文件会占用大量时间。

增加整个解决方案复杂性的第二个因素是成本。

约翰创建的神经网络需要资源，他可以在其中保存模型权重的存储资源，以及用于进行推理的计算资源，整个项目对约翰来说是一件昂贵的事情。

接下来他做了什么，他创建了一个 Android 应用程序。

他找到了一种方法，用他创造的这个巨大的模型在机器人上做出推论。

所以约翰现在面临一个新问题。

问题是这个模型非常庞大。

而且手机也不是很有能力存储和处理这么大的模型。

所以现在的约翰，是真的糊涂了。

他尝试了不同的技术来完成这项工作。

但是没有什么能解决这个问题。

这就是 TF 光线进入画面的地方。

在我们开始讨论飞行之前，我想解释一下什么是边缘设备。

所以边缘设备就是你使用的普通手机。

因此，如果您计划创建一些令人惊叹的基于 TensorFlow 的应用程序，那么基本上您可以利用的主要平台之一是您的手机，bat 或 Android 手机，或 iOS 驱动的手机，任何东西都可以。

其他硬件设备可以分为边缘设备或微控制器。

所以有一些惊人的应用程序是用非常小的计算能力构建的。

这一切都要归功于微控制器。

鉴于最近，你所使用的可变设备通过使用更多或更快的 CPU 增加了它们的计算能力，你也可以将你的智能手表等可穿戴设备放入边缘计算的行列。

现在让我给你一个边缘计算的正式定义。

因此，边缘计算基本上是将计算和存储资源移至更靠近所需位置的实践。

这就是你的边缘设备发挥作用的地方。

现在，如果您还记得，john 有两种选择来部署模型。

第一个选项是在云上部署模型。

现在，你们中的许多人可能会有这样的印象，即使用大型 GPU 在服务器上运行的机器学习模型比在设备本身上运行要好得多。

事实是，边缘设备已经成为机器学习的重要平台。

为什么它越来越受欢迎，以至于你必须在边缘设备上运行整个机器学习模型？好吧，让我一次分享一个细节。

边缘计算作为一个整体越来越受欢迎的首要原因是延迟。

需要实时速度的用例肯定需要模型在设备上运行。

例如，您可以将 resnet 50 的推理延迟从 30 毫秒减少到 20 毫秒。

但是网络延迟可能高达几秒钟。

所以本质上，这也取决于你的模型部署在哪里。

你在哪里等 API？

因此，如果您考虑到所有这些因素，那么当您想要接近实时或完全实时地进行推理时，在服务器上部署一个模型并不是最好的情况。

在边缘设备上创建机器学习模型很重要的第二个原因是因为网络和活动。

因此，如果你回到我们之前的例子，其中 john 想要创建他自己版本的 Google lense 来检测地标，如果他碰巧访问了一个几乎没有连接的国家，那么在设备上创建一个模型会比在服务器上部署它好得多，因为这是对网络的额外依赖。

创建在边缘设备上运行的机器学习模型非常重要的第三个原因是用户隐私。

在处理敏感用户数据时，将机器学习模型放在边缘设备上也很有吸引力。

云上的机器学习意味着你的系统可能必须通过网络发送用户数据，这使得它容易被拦截。

云计算还意味着将许多用户的数据存储在同一个地方或位置，这意味着数据泄露会同时影响许多人。

因此，创建可以在边缘设备上运行的机器学习模型变得非常重要，这可以保护用户的隐私数据。

现在让我向你们展示一些设备上机器学习用例的例子。

我现在向你们展示的第一个是在 YouTube 上使用 AR 试用各种化妆品的功能。

你在这里看到的整个计算过程，实际上是在设备本身上进行的。

现在我想给你们看的第二个例子是你们可能已经知道的，那就是谷歌翻译。

谷歌翻译有一个功能，可以让你用手机摄像头捕捉文本，并在没有任何互联网连接的情况下实时翻译它们。

使用边缘计算，更具体地说，使用 TF light，所有这些基本上都是可能的。

现在，您已经看到了您可以在自己的终端上创建的令人惊叹的新应用，您可能会想知道，john 最初采取的第二种选择是创建一个完整的巨大 TensorFlow 模型，然后从该设备运行推论。

为什么失败了？为了回答这个问题，当你创建混沌或张量流模型，并将它们直接部署到边缘设备上时，你可能会面临一些挑战。

边缘设备不仅仅限于手机，而且您的微控制器也具有有限的计算能力。

有限的记忆。

电池消耗也是你必须考虑的一个因素，还有应用程序的大小。

如果我认为一个简单的微控制器是好的，处理能力是如此之大，以至于你可以在一个 3 或 4gb 的模型上运行推论。

如果您考虑大多数边缘设备的存储容量，那么理想情况下，您不会有太多的存储可供一个型号使用。

这些也是约翰在采用第二种方法时面临的挑战。

那么解决办法是什么呢？解决方法是张量流光。

因此，flow light 是一个生产就绪的跨平台框架，用于在移动设备和嵌入式系统上部署机器学习模型。

而目前的 flow light 支持 Android、iOS 和任何可以运行 Linux 的物联网设备。

所以本质上，如果你手边有任何这些硬件设备，那么你可以快速创建一个张量流模型，将其转换为等效的 TF 光模型，并开始使用令人惊叹的 TF 光模型。

现在，你可能想知道，创建一个 TF 灯光模型的工作流程到底是什么？好吧，我们也来看看这个。

工作流程非常简单，首先创建一个张量流斜线 karass 模型。

因此，在整个过程中，您必须收集数据，您必须清理数据，预处理数据，然后创建模型，迭代多个模型，根据您所追求的指标，如果您追求更高的准确性分数，那么您将选择一个可能提供最佳准确性的模型。

就这样，你已经准备好了张量流模型。

现在，从张量流模型，你把它转换成张量流光模型。

所以会发生格式变化。

我会在后面的讨论中详细介绍。

一旦您将模型从 TensorFlow 转换为 TensorFlow light，您就可以继续部署整个 TF light 模型，并在边缘设备上运行您的推理。

当我说推论时，我指的是预测。

好吧。

现在让我用框图来解释一下。

这基本上就是我刚才提到的工作流程，你从高层次的好奇心开始，创建一个模型。

一旦你准备好模型，你就可以使用一个 TF 灯光转换器了。

转换器基本上是把你保存的 TensorFlow 格式文件，转换成一个平面缓冲文件。

我将给你一个关于我所说的平面缓冲文件的概念。

所以让我们继续前进。

因此 TensorFlow lite 以平面缓冲格式表示您的模型。

现在，flat buffer format 是一个高效的跨平台序列化库，适用于 c++、C sharp、go Java、kotlin、JavaScript、Python 等等。

它最初是在谷歌为游戏开发和其他性能关键的应用程序而创建的。

但是慢慢地，Google 意识到你可以在 edge 设备上部署模型时使用平面缓冲格式。

现在你可能会有一个明显的问题，为什么不使用我们旧的经过测试的协议缓冲区。

为什么要转向最新的 flatbuffers 协议缓冲区呢？只是给你一个背景，你创建的所有 kiraz 模型，你创建的所有 TensorFlow 模型，本质上都是协议缓冲区格式。

协议缓冲区本质上非常类似于 Google 创建的平面缓冲区格式。

主要区别在于，在访问数据之前，flatbuffers 不需要对二级表示进行解析或解包。

并且在协议缓冲器的情况下，代码实质上也更大。

在使用 TF lite 时，我们使用平面缓冲格式，而不是协议缓冲格式。

所以现在让我们继续前进。

到目前为止，我们已经了解了边缘计算的各个方面，我们已经了解了在边缘设备上部署模型比在云上部署更好。

我们还看了什么是张量流光，以及在这个时间点上它能支持什么。

现在，当我通过代码向您展示 TF light 如何在不影响精度的情况下压缩您的模型尺寸时，事情就变得有趣了。

那么现在就让我们一起前去见证 TF 之光的神奇吧。

现在我们已经了解了 TensorFlow light 和边缘计算的基础知识，让我使用 Python 向您展示 TensorFlow light 的强大功能。

对于这个例子，我用的是 Google collab。

对于那些不知道的人来说，谷歌实验室是一个在线环境，你可以在其中编写 Python 代码，你可以创建机器学习和深度学习模型，你也可以利用深度学习模型，谷歌让你在相当长的时间内访问这些模型。

这是我将要使用的界面。

我将在视频的描述部分附上 GitHub 库的链接，可以从那里自由访问代码。

也在 GitHub 存储库中。

我也给你一个可以直接打开 Google collab 笔记本的链接。

所有的基础工作都完成了，现在让我继续向你展示张量流光的魔力。

因此，我将在本教程中遵循的过程是，我将使用张量流斜线 Eros 创建一个深度学习模型。

我会把它缩小到一个相当于 TF light 的模型。

一旦完成，我将向您展示原始模型和压缩模型之间的大小差异。

我将向您展示如何在不牺牲精度的情况下进一步压缩模型的技术。

现在，让我通过点击 Connect 在 Google collab 上创建一个实例。

所以目前，谷歌正在为我的计算分配一些空间。

如果你打算在你的本地机器上复制我今天视频中展示的全部内容，那么你也需要一些安装。

鉴于我正在使用 Google collab，我在这个例子中需要的所有依赖项都已经满足了。

现在让我来看一下整个教程需要的不同的东西。

首先，我需要语音模块来读取我的文件。

接下来，我将把 NumPy 作为 NP 导入，我将需要这个特殊的库来进行数学运算。

我也将需要 h5 p y 库。

h5 p vi 库是 HD f5 二进制数据格式的 pythonic 接口。

所以从技术上来说，无论我在混沌中创建什么模型，我基本上都会将其保存为 h5 格式。

接下来，我需要 matplotlib。

这再次用于可视化。

我需要 TensorFlow，我会导入 kiraz。

From TensorFlow.

当我们进入深度学习模型创建方面时，这些是需要的一些层。

如果我想计算我的模型在准确性分数方面的表现，这就是 SK learn 点阵模块将为我提供准确性分数功能的地方。

从系统库中，我还需要函数 get size。

这些是我需要的一些东西，为了创建一个类似 TF 的模型。

现在，让我来运行这个单元。

所以当我运行这个单元时，Python 实际上运行了这段代码。

现在让我来运行这个单元。

所以我没有看到任何错误。

这意味着我们所有的进口都到位了。

让我向你们展示一下我在这个教程中使用的 TensorFlow 版本。

我目前用的是 TensorFlow 2.6 点零。

如果 API 发生了变化。

请随意参考 TensorFlow 文档。

有两个函数创建了第一个函数名是获取下划线文件的下划线大小。

本质上，我是在使用 OAS 库传递文件位置，特别是函数 get size，我能够获得我传递的特定文件的大小，以字节为单位，现在让我继续运行这个单元格。

在前面的函数中，即获取下划线文件的下划线大小，返回的值将以字节为单位。

现在，我没有理解以字节为单位的文件大小的值，而是创建了一个名为 convert 下划线 bytes 的帮助函数，它主要接受以字节为单位的输入大小，并将其转换为 KB/MB。

这是我创造的东西。

我没有包括 DBS，因为这更像是一个解说视频，我打算创建一个更小的模型，或者更确切地说是一个概念验证，而不是一个巨大的模型。

所以这就是为什么我把我的部队限制在卡维斯或 MDS。

所以让我来负责销售。

对于这个特殊的例子，我将基本上使用一个非常著名的数据集和深度学习，Carla 的时尚 m&s 数据集，让我取消隐藏单元格，这样时尚 emnes 数据集包含 70，000 张灰度图像，它们属于 10 个不同的类别。

因此，分类将包括衬衫、裤子、套头衫、连衣裙、码、凉鞋、衬衫、运动鞋、自行车和踝靴。

这些是这个数据集的不同类别。

整个活动是一个有监督的学习任务，我会有一组图像，每个图像都会有一个与之关联的标签。

我正在尝试训练一个深度学习模型。

所以现在让我继续。

所以你不必担心下载部分。

嗯，如果你已经正确安装了 TensorFlow，那么本质上你只需要调用混沌点数据集点时尚大赦函数，将整个数据集保存到一个名为时尚下划线 m nest 的变量中。

这是我做的第一步。

一旦你做到了这一点，那么本质上你下一步要做的就是把你的数据集分成训练和测试。

实现这一点的理想方法是调用一个名为 load underscore data 的函数。

这就是函数的内容。

一旦您从刚刚创建的变量 fashion 下划线 m 结束调用此函数，您就能够将数据拆分为训练图像、训练标签、st 图像和测试标签。

原来就是这么简单。

所以让我来运行这个单元。

所以我们已经下载了数据文件，我们已经把数据分成了训练和测试。

我还创建了一个变量，一个列表变量，名为 class underscore names，它包含了作为整个活动一部分的所有类的名称。

所以让我来运行这个单元。

如果你还记得，我们的数据集中有 70，000 个样本，我们已经将整个数据集分成了训练和测试。

现在让我向你们展示有多少图像是训练数据集的一部分。

所以让我来管理这个牢房。

所以我的训练数据集的形状是 6 万个逗号 28，逗号 28。

所以我有 6 万张照片。

每个图像的大小为 28×28。

所以 28 行，28 列代表每张图片，我有 60，000 张这样的图片。

鉴于这是一个监督学习任务，我还需要 60，000 个标签。

让我检查一下我的训练数据集中的标签总数是否是 60，000。

所以让我来管理这个牢房。

你可以清楚地看到，我也有 60，000 个标签。

鉴于此，我已经提到有 10 个独特的类，让我也验证一下。

所以你可以清楚地看到，我的课程号从 0 到 9。

这个映射就是我在这里创建的，它包含在变量类下划线名称中。

现在，让我们继续探索测试数据集。

让我快速地取消隐藏，让我给你们看一下测试数据集中图像的总数。

所以 1000 年的培训费用是 10000 到 60000。

为了测试，每个图像的尺寸也是 28×28。

类似地，如果我查看测试下划线标签，我将有 10，000 个样本。

接下来我想给你们看的是一个样本图像。

让我展示给你看。

这是一个样本图像，是这个数据集的一部分。

这显然是一个踝靴。

图像的大小是 28×28。

你在这里看到的是 28 行 28 列。

在我们前进并训练神经网络之前，一个好的做法是缩放图像的强度值，其范围在 0 到 255，0 到 1 之间。

这就是我在这段代码中所做的。

让我来运行这个。

所以现在我的火车图像的值范围是从 0 到 1，而不是从 0 到 255。

到目前为止，我们已经下载了数据集，我们已经将数据集分为训练和测试。

我们也做了一些预处理，现在是我们创建一个简单的神经网络的时候了，它将整个图像分类到 10 个类别中的一个。

所以在这段代码中，我从混沌库中调用顺序类。

我将第一层作为展平层传入。

如果你还记得的话，图像是 28×28。

如果我必须通过一层，那么我必须首先基本上将其展平，我不会创建卷积神经网络鉴于数据集相当简单，我将坚持使用正常的深度神经网络。

因此，我添加的第一个层是一个扁平层，其中我传递的输入形状是 28，逗号 28。

第二层是致密层。

并且提供给该密集层的激活是 relu。

最后一层又是一个密集层，因为我有 10 个不同的类要分类。

这就是我在这里看到的。

让我快速创建一个模型实例。

让我来运行这个。

在我们继续编译模型之前，我也将向您展示模型的结构。

所以我就说模型点汇总。

这基本上是模型总结。

对于给定的架构，我们有近 100k 个可训练参数。

现在下一步是编译模型。

我传入优化器，我传入过去的分类交叉熵损失。

考虑到我们的类是互斥的，我使用稀疏分类交叉熵损失来对比正常分类交叉熵损失。

而母体为他的准确性而关注外壳。

所以我想创建一个相当准确的模型。

现在让我来运行这个单元。

所以我创建了模型的一个实例。

我也已经编译了模型，现在是时候传入训练图像和训练标签来训练整个可训练参数了。

现在让我调用模型点拟合函数，其中我将传入火车图像，火车标签，我将整个练习运行 10 个时期。

所以让我来管理手机。

因此，随着每个历元的增加，你可以看到精度在增加。

因此，我们已经成功地训练了我们的模型，我们已经达到了大约 91%的训练准确率，这是相当不错的，因为我只训练了 10 个时期的模型。

所以让我们向前看。

记住一件事，这个视频的目的不是在这个时候训练最准确的分类器，而是向你展示 TF light 的力量，所以这就是为什么我停在了 10 英镑一盒。

现在我要做的下一件事是创建一个名为 SK Ross 下划线模型下划线名称的变量。

这将被用作参考，或者这将是我稍后评估 TF 轻型模型的基线模型性能。

所以这个变量的名字是 pf 下划线模型，下划线时尚下划线 m NIST 点 h phi。

让我快速运行一下这个单元。

现在让我继续调用模型点保存函数，并传入我刚刚创建的文件名。

所以让我来管理手机。

所以当你运行这个单元的时候，你将会有一个文件，这个文件将会在你的 Google collab 会话或者你的本地目录中创建，本质上就是你保存的模型文件。

让我证明这是好的。

这是我们已经创建的保存的模型文件。

所以让我很快和我的细胞了。

接下来，我将向您展示我们创建的这个特定文件的大小。

让我调用我已经创建的两个函数，即转换下划线字节，和获取下划线文件下划线大小，我传入相同的文件名，我希望文件大小以 MB 为单位。

所以让我来管理手机。

所以目前，我有一个占用 1.2 Mb 的模型。

接下来，我将创建一个名为 SK Ross 下划线模型的变量，下划线大小，并将等效字节大小保存到这个特定的变量中。

所以让我来管理手机。

我们知道，这个模型在训练数据集上表现得非常好。

但是，关键的试金石测试是检查模型在看不见的数据上表现如何，这些数据是我的测试数据集。

让我来评估一下我们的模型在他们的测试数据集上的表现有多好。

所以我调用函数模型 dot evaluate，我传入测试图像，测试标签。

我将结果保存到两个变量中，称为测试下划线丢失和测试下划线准确性。

让我快速运行一下这个单元。

所以你可以清楚地看到，损失非常小，大约在 3，7 点左右。

我已经达到了大约 88%的测试准确率。

我们已经完成了第一部分，现在是时候进入下一部分了，那就是创建一个相同模型的 TF 灯光。

所以让我们向前看。

因此，我通过创建一个名为 TF 下划线轻下划线模型下划线文件下划线名称的变量来开始活动。

我给这个特定的 TF light 模型传入了一个等价的名字，在我们的例子中，现在是 TF 下划线 light 下划线 model.tf light。

让我快速运行一下这个单元。

现在，将 TensorFlow 模型或 karass 模型转换为 TF light 模型的过程基本上只需要几个步骤。

这就是我现在要强调的。

所以第一步是从 Kara 的下划线模型调用 tf.light.tf 灯光转换器 dot，我传入我创建的模型。

如果你还记得模型变量的名字，本质上是 model，这就是我在第一行传递的。

一旦我从 Eros MADI 创建了一个 TF light converter 的实例，我将整个文件保存到一个名为 TF 下划线 light 下划线 converter 的变量中。

最后，我接下来要做的是调用 Convert 函数。

一旦转换发生，我希望将结果保存到一个名为 TF light 下划线模型的变量中。

让我快速运行一下这个单元。

所以，如果你看看输出，它说资产被写入一个特定的临时文件。

因此，从这个临时文件中，我基本上必须检索模型权重，并将其保存到一个 TF light 等效文件中。

这就是我用这段代码所做的。

我已经创建了第一个变量，它是 TF light 下划线模块下划线名称。

我会传入我在这个部分的第一行中创建的名字。

我用写权限打开文件名，将这个特定的临时文件写入我创建的这个文件中。

原来就是这么简单。

让我快速运行一下。

因此会显示一个特定的输出。

这告诉我已经返回到这个特定文件的总字节数。

现在让我向您展示这个 TF lite 模型的确切大小，单位是千字节。

让我来运行这个。

所以整个文件大小接近 400 千字节。

所以我们从一个大约占用 1.2 Mb 的模型开始。

在运行了几行代码后，我们将文件大小降低到了 400 kb 左右。

现在，让我把这个文件大小保存到一个名为 s TF light 下划线文件下划线大小的变量中。

一旦我们向前发展，这将是非常有意义的事情。

让我快速运行这个 l。

现在我们已经将一个模型从 kiraz 转换为 tf light。

但是我们目前还没有验证的一件事是这个模型在性能方面有多好。

它对看不见的数据真的有用吗？还是在准确率上有所下降。

所以这就是我接下来要检查的，在使用 TF light 压缩模型后，我们是否会损失准确性。

因此，在本节中，我将介绍如何根据 TF light 模型的性能来验证结果。

现在，让我快速地取消隐藏单元格。

现在，不要被这段代码吓到，我会帮助你理解我想要达到的目标。

现在，将模型、TensorFlow 或 karass 模型加载到 TensorFlow 会话中相当容易。

但在这里，我们所做的是我们已经创建了一个 TF 光模型。

如果回到我们之前的讨论，TF led 模型本质上是平面缓冲格式文件，而不是普通的协议缓冲文件。

因此，为了让我们在 TensorFlow 或 Python 会话上从 TF light 文件中做出推断，我们需要一个称为解释器的东西。

因此，你将基本上利用张量流解释器来加载 TF light 文件，然后做出推论或预测。

现在让我带你们看一下每一行代码。

所以在第一行中，我创建了一个解释器类的实例，我传入了我们刚刚创建的 TF light 模型名称。

因此，如果您查看这一特定部分，您还会看到一个 TF lite 文件。

这就是我要传递的信息。

现在，一旦我们创建了解释器对象，解释器对象就保存了关于模型的细节。

它会有关于输入的详细信息，它所期望的值，它所期望的值的类型，在输出方面，它会告诉你输出应该是什么形状。

就输出而言，它将告诉您输出形状以及输出类型，即输出值，它将预测哪些是值，哪些是值的类型。

所以这就是这个特殊的解释器，实际上会有的细节。

它获取的细节也是来自我们创建并传入 TF lite 文件的解释器对象。

因此，所有的细节都将被捕获到这个特定的 TF lite 文件中，这就是解释器对象所读取的内容。

这就是我们试图从输入下划线细节和输出下划线细节中积累的内容。

一旦我们有了输入和输出的细节，我也对预期的输入的形状感兴趣，并且输入的类型是那里的输入的可变性质。

所以让我快速运行这个单元，让它更有意义。

所以仔细看，输入形状是 128 ^ 28。

它期望的输入类型是 NumPy，float 32，输出形状是一个逗号 10，也就是一个，所以基本上有一行 10 列，输出类型也是 NumPy float 32。

这基本上就是 TF lite 文件包含的内容。

现在如果你看这个特殊的，这表示 A 航班一次期望一个输入。

现在我想检查它在 10，000 次输入时的性能。

这就是我必须将输入形状重新整形为一个特定值的地方，这就是我在这段代码中要实现的。

再次重申，输入形状是 128 ^ 28。

所以理想情况下，我只需要传入一个图像样本。

本质上，我会得到相应的输出。

但本质上，在我的用例中，我想验证 TF light 模型对于我所拥有的测试数据集(基本上包含 10，000 张图像)的表现有多好。

所以如果你清楚这个想法，让我们继续。

所以现在我想验证我的 TF light 模型在我的测试数据集上的表现有多好。

所以我把调整下划线张量的大小称为下划线输入函数。

我传入了我想要调整这个特定索引值的细节，并传入了我必须如何调整它的大小。

所以目前，我有一万个样本。

这就是我在这里输入的，10，000 个逗号 28，逗号 28。

类似的调整大小操作是我在输出端做的。

所以你可以看到你的一万个逗号 10，从最初的一个逗号 10。

这就是我在这里所做的。

现在，一旦调整操作发生，我想调用分配张量。

来改变解释器的整个结构。

这是使用 TF light 文件读取的内容。

现在，当我打印输入详细信息和输出详细信息时，我应该能够看到整个 TF light 输入输出值已经更改。

让我快速运行这段代码。

所以你可以清楚地看到，输入形状从 128 28 变成了 10，028 28。

因此，这将有助于我验证我的 TF light 模型的性能。

我现在想强调的另一件事是 s 下划线图像点 d 类型是浮点 64。

因此，如果您查看模型预期的输入形状是 NumPy 点 float 32。

现在，为了验证我的 TF light 模型，我必须做的另一个更改是，我必须创建一个新的数组，名为 test understand images understand NumPy。

传入原始数组，改变 D 类型。

我也可以在同一个地方做。

但是我选择创建两个不同的数组。

让我快速运行一下这个单元。

所以现在如果我给你看 D 类型的测试下划线图像点 NumPy，它将是 NumPy float 32。

现在我们已经为我们的输入集正确设置了整个解释器对象，这就是测试数据集。

我现在要做的就是首先调用 set 下划线张量函数，传递我刚刚创建的测试下划线图像下划线数字数组，然后调用 invoke 函数。

invoke 函数主要做的是传入输入，获取输出。

准备好输出后，调用 get 下划线插入函数，它会为您准备好输出，并将其保存到一个名为 TF 幻灯片下划线模型下划线预测的变量中。

让我快速运行一下这个单元。

现在，您在这里看到的输出，即预测结果形状是 10，000 行和 10 列。

所以每一列本质上都包含一个概率得分。

所以我接下来要做的是，我必须选出 0 到 9 之间的值，或者说索引，它有最大的概率，这就是我用这个叫做 NP 点 arg max 的函数所做的。

这将帮助我直接得到从 0 到 9 的数字，而不是有 10 个不同的概率分数栏。

现在让我来计算准确度分数。

让我给你打印出来。

因此，当你将 TF 轻型模型与普通的 karass 模型进行比较时，你会发现它的测试精度是完全相同的。

在整个过程中，你节省了多少空间？说，梅尔让我计算一个文件大小之间的 TF 建兴和 karass 模型文件大小的比例。

所以总的来说，TF lite 模型占据了我的普通 karass 模型所占据的整个文件大小的 32%。

但独特之处在于，我没有失去任何准确性。

所以这就是 TF Lite 的强大之处。

我已经能够将我的整个模型从 1.2 Mb 压缩到大约 400 kb。

我还没有在准确性上妥协一点。

这不是很神奇吗？好吧，如果你认为故事到此结束了，等一下，还有更多。

到目前为止，我们所做的是我采用了一个张量流模型。

在没有任何优化的情况下，我基本上把它转换成了一个等效的 TF 光模型。

现在我将向您展示如何进一步压缩您的模型。

而不损失准确性。

现在让我给你们介绍一个新概念，叫做 les 量子化。

我刚刚提到的这个术语到底是什么，那就是量子化。

那么对于一个给定的可以用浮点 32 或者浮点 64 格式表示的权重值呢？如果我们可以缩小这些特定值的大小，并且在精确度上几乎没有变化，那不是很好吗？这本质上是量化的概念，我减少了每个权重值的总位数，这样整个数组的总大小就减小了。

更清楚地说，如果我有一个类似这样的神经网络，这个特定的权重值是 5.31345，这个特定的权重值是 3.8958。

你还有另外一种方法，如果我能把这些占据很多位的表示法改成这样，精度会有一点点下降。

但总的来说，我将能够进一步压缩我的模型。

无论你有什么问题，只要等待一段时间。

如果你清楚这整个想法，让我们回到编码部分。

我将向您展示如何进一步压缩您的 TF light 模型。

因此，默认情况下，在前面的示例中，我们采用了 karass 模型，并将其转换为 TF light 模型，每个权重值基本上都是 float 32 格式。

如果我把它从浮点型 32 压缩到浮点型 16 不是很好吗？

这是我接下来要进行的活动。

所以我创建了一个变量，它的名字是 MTF 下划线轻下划线模型下划线浮点下划线 16 下划线文件下划线。

我基本上给它一个 TF 闪电，它基本上代表了里面的全部重量是流体 16。

这就是我在这里看到的。

让我快速运行一下这个单元。

如果你看一下上一节我们如何创建一个 TF 灯光模型，第一行代码对你来说是非常熟悉的，你传入你的 karass 模型，你从 kiraz 模型调用 TF 灯光转换器 dot，你把它保存到一个变量中。

甚至最后一段代码也是你已经看过的。

你目前还没看到的是优化。

因此，当您创建 TF light 转换器的实例时，有一个名为 optimizations 的标志。

这里有优化标志。

我将它设置为 tf dot light dot optimizer default，所以我希望进行默认优化。

我在这里做另外一件事。

因此，我将在下一节详细介绍优化。

所以也要坚持这个想法。

现在这里又多了一个称为目标下划线规范的标志，以及支持的下划线类型。

这里是我设置从 floor 32 到 float 16 的每个权重值的地方。

这就是我在这里做的事情。

让我快速运行这段代码。

现在我有一个 TF light 模型，其中每个权重值都是一个 float 64 Automat，我再次遵循相同的过程，其中我从临时文件中获取数据并将其保存到 TF lite 模型中。

让我来运行这个。

我不知道你是否已经猜到了。

对于新转换的飞行模型，这实质上是一个以字节为单位的文件大小。

因此，如果我现在向你展示这个新转换的 TF 飞行模型的大小，那么我的大小已经从 400 千字节急剧减少到 200 千字节。

在这里，我唯一改变的是，我改变了每一个创造价值的个体表现。

这是不是很神奇，我可以节省这么多的内存，只需要在这里和那里改变一些值。

再说一遍，将文件大小保存到一个名为 TF lite 下划线 float 下划线 16 下划线文件下划线大小的变量中。

让我来运行这个。

现在如果我把它和原来的 karass 模型比较，这个特殊的模型占据了原来模型的 16%。

如果我将其与之前创建的版本进行比较，那么我可以看到，通过将权重从 32 层更改为 16 层，我可以实现近 50%的压缩。

所以这就是优化和 TF light 的力量。

如果您认为这很奇怪，那么在下一节中，我将进一步压缩这个模型。

我现在没有给你们看精度图，你们可能会想，为什么他没有给我们看精度图呢？答案是否定的。

我向你展示一个均匀压缩模型的准确性。

这将让您大致了解压缩对整体精度值的影响。

好吧。

现在我们已经到了最后一部分，我将进一步压缩这个模型。

好的，这里我创建了一个变量，叫做 TF 下划线光，下划线大小，下划线当前下划线模型，下划线文件下划线名称。

这里我想看看有这个特殊名称的 eflite 文件。

所以我会快速运行单元。

在前面的示例中，我将每一个重量值从 32 楼更改为 16 楼。

与其让你决定什么对你的模型好，我宁愿让 TF light 替我决定。

因此，如果您一直在跟进，那么这段代码就是我们已经讨论过的内容。

这段代码也是我们已经讨论过的内容。

这是独一无二的。

所以在这里，我设置了优化飞行。

在这里，我刚刚提到，优化大小，有不同的值，你可以在文档中浏览。

因此，根据您的需求，您可以对尺寸进行优化，其他优化也适用于 TF light。

所以我没有指定我想要什么类型的数据类型，我只是想要一个优化程度最高的版本，其中这个特定的 TF light 模型所占用的大小是压缩程度最高的版本。

好吧，那我就快速检查一下手机。

所以我捕捉所有保存在临时变量中的文件，我把它保存在我创建的这个特殊变量中。

如果你已经猜到了，这是新的文件大小，以字节为单位。

如果我转到千字节部分，那么我的文件大约占用 100 kb。

如果你记得，我们从 1.4 Mb 开始，我们已经降低了深度神经网络的文件大小。

两百千字节。

这不是很神奇吗？

再给你一些数字，如果我将这个特定的文件大小与我的原始文件大小进行比较，那么我当前的文件几乎是原始文件大小的 8%,这是我创建的 karass 模型。

如果我也将其与之前的模型进行比较，我基本上能够实现 50%的压缩，这都是因为针对大小进行了优化。

这本质上是一种 TF 光的力量。

现在，我对压缩非常满意，我有一个 1.4 Mb 的文件，已经压缩到 100 kb 左右。

但是精度还是一样吗？我们将再次遵循相同的过程，其中我将新的量化模型加载到解释器对象中。

我将获得这些对象的详细信息，我将再次对值进行整形，并通过解释器对象将测试数据集完全传递给它。

所以我会快速运行单元。

所以你可以清楚地看到，128 ^ 28 是解释器对象的输入值，我的输出是一个逗号 10。

预期的输入和输出值是 NumPy，float 32。

所以这也很好。

到了最后一部分，我遵循同样的过程。

同样，流程没有变化。

我有一个包含 10，000 张图片的测试数据集，这就是我在这里传递的内容。

我分配张量，我得到细节，我也会把细节展示给你们。

所以让我快速运行这个 100282812828。

因此，我们已经调整了张量输入值的大小，我们希望验证我们的测试数据集。

你基本上不需要再做这一步，而是从最初的部分复制它。

所以我再运行一次。

我再次传入这些值。

现在我计算准确度分数。

现在是这个高度压缩的 TF 轻型模型的试金石。

让我快速运行一下这个单元。

因此，占据原始 karass 模型大小的几乎 8%的 TF lite 模型的精度相当于原始 karass 模型。

如果我上去了，我就一口气录下了这段视频。

如果我上升，这个值去了哪里？这里的价值是 7.66%。

这里是 87.59%。

这就是你使用 TF Lite 可以达到的效果。

我从一个非常简单的神经网络开始，整个模型占用大约 1.2 Mb。

你也可能认为 1.2 Mb 太小了。

但是问题陈述相当简单。

如果你有一个非常复杂的例子，其中你必须将图像分为 1000 或 10000 个类别，模型大小最终会增加。

那么目标就变成了我们能压缩模型大小吗？答案是肯定的，TF lite 将帮助您压缩模型大小，而无需在准确性方面做出任何让步。

如果你已经到了这一步，那么我假设你已经看完了整个视频，或者如果你随机地到了这一步，无论你以哪种方式到了这一步。

我希望你喜欢今天的视频，我一直在制作关于数据科学、机器学习和 Python 的精彩视频。

因此，也可以在视频的描述部分随意查看我的频道。

非常感谢您观看这段视频。
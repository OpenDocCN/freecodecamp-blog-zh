# 人工智能的下一步——星际争霸 2

> 原文：<https://www.freecodecamp.org/news/the-next-step-towards-artificial-general-intelligence-starcraft-ii-f562d5607e2/>

丹尼尔·伯克

# 人工智能的下一步——星际争霸 2

![01DyVV06IHibamfEFqejIt37BpSLcsqnqvu5](img/fd99c462bebd5fde17a431d36feec89d.png)

A combination of two behemoths.

我正在攻读我自己自创的[人工智能硕士学位](https://medium.com/@mrdbourke/my-self-created-ai-masters-degree-ddc7aae92d0e)。来自 [DeepMind](https://deepmind.com/) 的创作让我着迷。当他们为了推动人工智能(AI)研究向前发展而推出游戏界最大的名字之一的混音带时，我会听。

在我们进入这次合作的细节之前，先快速回顾一下人工智能和游戏的历史。

AlphaGo 通过[引入违背数百年游戏策略的棋步](https://www.youtube.com/watch?v=JNrXgpSEEIE)震惊了围棋界，同时击败了几名世界冠军。[深蓝](http://www-03.ibm.com/ibm/history/ibm100/us/en/icons/deepblue/)在 1997 年的国际象棋比赛中也是如此，击败了当时的世界冠军加里·卡斯帕罗夫。

一台计算机在 1997 年击败了一名国际象棋世界冠军，为什么直到 2016 年才征服围棋？为什么是现在呢？

让我解释一下情况。

在国际象棋中走了 4 步(2 步走白棋，2 步走黑棋)后，可能的棋盘组合数是 8，902。总的来说，可能的棋盘组合比可观测宇宙中的电子还要多。但是在国际象棋中，合理的走法总数(例如，不必要地牺牲一个王后作为棋子)要少一些，大约是 10 ^ 12 亿或 10 后面跟着 40 个零。

> 40,000,000,000,000,000,000,000,000,000,000,000,000,000

对于 1997 年最快的超级计算机来说，计算每一个可能的棋盘布局(结束游戏)将采取的准确的获胜步骤，太阳将会吞噬地球几次。显然，像这样的暴力方法是不可行的。

#### **深蓝是怎么做到的？**

“深蓝”使用了一个选择性系统，该系统会在选择一系列要探索的棋步之前评估棋盘的状态。没有最大化成功概率的棋被淘汰。

这种结合并行处理的选择策略让深蓝在三分钟内计算出 600 亿个可能的走法，这是古典国际象棋中每个玩家走法的时间。

这种权力导致卡斯帕罗夫在废黜后指责 IBM 作弊。

#### 为什么围棋又花了近二十年才攻克？

没有规则，我们来对比一下两个游戏板。正如你所看到的，国际象棋棋盘上的彩色方块看起来很花哨，但是围棋棋盘上的方块是它的五倍。

![aUR8n0utFjV3a07XPNdchlOeSCNdB577kbo-](img/b9c4582445a58e095104ac355fd05847.png)

An 8x8 Chess board versus a 19x19 Go board. Thank you [Stuart French](https://www.slideshare.net/kurokaze204/cpx-2011-complexity-and-the-game-of-go) for the image.

还记得国际象棋中前四步之后有 8902 种可能的走法吗？在前三步棋之后，围棋有 46，655，640 种可能的走法。19x19 棋盘上合法围棋位置的数量计算如下:

> 208,168,199,381,979,984,699,478,633,344,862,770,286,522,453,884,530,548,425,639,456,820,927,419,612,738,015,378,525,648,451,698,519,643,907,259,916, 015,628,128,546,089,888,314,427,129,715,319,317,557,736,620,397,247,064,840,935.

好吧，真的很大的数字，计算能力自 1997 年以来增加了很多，围棋一定很容易上手。

不完全是。

当你考虑到增加棋盘的影响力和占领不确定的领土而不是试图占领另一个棋手的王时，围棋变得更加复杂。

即使发挥了摩尔定律的全部力量，蛮力也不是征服围棋的选择。

#### AlphaGo 是怎么做到的？

三种技术的结合:高级树搜索、深度神经网络和强化学习。

树搜索是人工智能中用来寻找目标最佳路径的一种流行技术。想象一下，你在一棵圣诞树的顶端，你的目标是在几根树枝下找到一个蓝色的装饰品，但是，你不知道它在哪根树枝上。为了找到装饰品，你必须搜寻树枝。

![GmujXeycHJmQUDITr6ToNgVUdM8ayhu3ieyD](img/c4f1245ca8c06016a6ac07e4e68e78cf.png)

A merry explanation of tree search. Tree image [source](http://www.clipartpanda.com/clipart_images/xmas-christmas-tree-5-png-1268212).

深度神经网络包括获取大量输入数据源，并对其执行若干数学变换。这导致输出数据源更小，但仍在与输入数据相同的概率分布内。

例如，假设您有 100 万个过去如何寻找蓝色装饰物的例子，这将是您的输入数据源。输出可以是找到蓝色装饰物的一组最佳和最有效的模式。

![-zUlbTv3eBL3DoINMmFpx6OKEbJ5NQKsgyQd](img/a6ca7690142f30deb82652319c4e0a36.png)

A simple overview of Deep Learning. Tree image [source.](http://www.clipartpanda.com/clipart_images/xmas-christmas-tree-5-png-1268212)

对于 AlphaGo，将圣诞树的顶部替换为您在围棋棋盘上的当前位置，树枝是您不同的走法选项，蓝色装饰物是最佳的下一步走法。

如果你曾经因为你的狗听从命令而给了它一点奖励，你就已经实践了一种强化学习的形式。一开始，AlphaGo 被展示了数百万个人类如何下围棋的例子，因此它可以建立一个基础水平的游戏。当 AlphaGo 在训练下棋时，它会因为走好棋而获得奖励。

将这些技术和强大的计算能力结合起来，就能产生一个非常优秀的围棋手，世界上最好的围棋手。

**围棋被征服了，下一步是什么？**

[星际争霸 2](http://us.battle.net/sc2/en/)。

星际争霸 2 是一款即时战略游戏。玩家建立军队，并希望占领战场。但是不要让这种简单的描述欺骗了你。

如果你认为围棋比象棋更上一层楼，那么《星际争霸 2》将这个数字提高到了 11。

### 星际争霸 2 为什么会有这么大的进步？

首先，军队可以包含各种不同的角色，游戏界面是彩色的。象棋和围棋只有两种文字和颜色，黑和白。

战场并不是完全可见的，除非玩家探索了那个区域，否则部分地图是隐藏的。想象一下，在国际象棋中，如果你只能看到你那一方，你就要计划一步棋。

延迟信用分配——有些动作直到游戏后期才会得到奖励。国际象棋和围棋都有这个功能，但远不及星际争霸 2 的水平。

对手可以是一个，也可以是很多。象棋和围棋都是一对一的。想象一下，在国际象棋中试图同时与三个人较量，但这三个人都有不同的规则。这就相当于在星际争霸 2 中迎战不同的军队。

这些因素使得星际争霸 2 确实是一个有价值的努力。但是这有什么意义呢？

### 为什么要创造智能系统来玩游戏？

DeepMind 的目标是解决智能问题，并利用它让世界变得更美好。创建能够学习解决复杂问题的系统是实现这一目标的基本步骤。

进入游戏。

游戏是非常可重复的状态。这意味着我可以玩你正在玩的游戏，我们都可以理解赢得比赛需要什么，以及成为一名好球员意味着什么。随着游戏开发和技术的进步，它们也变得越来越复杂。

即使游戏开发越来越复杂，游戏的一个基本方面仍然存在，即解决问题的需求。

游戏为重复解决问题提供了丰富的机会。围棋被认为是一个挑战，因为你在上面看到了很大的数字。这些大数字没有传达的是，它们都是问题的解决方案。

学习玩游戏的系统似乎是在浪费时间。但是这些系统不是在玩游戏，而是在学习解决问题。

创建学习玩游戏的智能系统，如围棋和星际争霸 2，是创建适应终极游戏，现实生活的系统的关键一步。

外面的世界比任何游戏都要复杂得多，但它仍然由一系列解决问题的过程组成。每天醒来，你必须解决如何去洗手间的问题，解决决定早餐吃什么的问题。我们已经习惯了这些事情，因为我们已经做过成千上万次了。当我们面对一个以前没有解决的问题时，难度就增加了。

一旦智能系统一遍又一遍地学习解决一个问题，它就慢慢失去了它是智能的形象。对于 AlphaGo 来说，这种情况正在发生。

人类有能力将他们解决问题的能力从一个领域转移到另一个领域。到目前为止，智能系统在这一领域落后了。

我们知道 AlphaGo 比任何其他人类都擅长下围棋，但它能学会骑自行车吗？人类可以很容易地从下围棋变成骑自行车。AlphaGo 不行。

为了实现这种迁移学习的能力，或者一些人可能称之为人工通用智能(AGI)，智能系统必须学会解决新的和更复杂的问题。

进入[星际争霸 2 学习环境](https://deepmind.com/blog/deepmind-and-blizzard-open-starcraft-ii-ai-research-environment/) (SC2LE)。

DeepMind 与暴雪(星际争霸 2 的制造商)合作，发布了 SC2LE，目标是在一个不是专门为此设计的游戏中催化人工智能研究。

你可以把 SC2LE 想象成一个健身房，智能系统可以去那里训练，希望能够打败职业人类玩家。

SC2LE 中可以找到的工具包括暴雪开发的机器学习 API，允许研究人员更深入地挖掘游戏机制，60，000+游戏重播的初始数据集，以及由 DeepMind 创建的开源 Python 库 [PySC2](https://github.com/deepmind/pysc2) ，以利用暴雪的功能层 API。

暴雪和 DeepMind 的一篇联合论文展示了一些令人惊讶的结果。即使是 DeepMind 实验室最好的问题解决系统也无法完成哪怕一个完整的星际争霸 2 游戏。这包括 DeepMind 制作的深度强化学习算法，该算法在 2015 年在 49 个不同的雅达利游戏上取得了超人的分数。

即使在完整游戏的简化版《星际争霸 2》迷你游戏(SC2LE 发布)中，原始论文中的智能系统也没有一个达到接近人类职业选手玩同样游戏的分数。然而，一些代理人在简单的迷你游戏中取得了与新手玩家相当的成绩。

这些初步发现令人兴奋。事实上，目前的智能系统甚至无法在简化版的星际争霸 2 上产生最佳结果，这意味着还有很大的改进空间。

SC2LE 和联合论文的发布为 AI 研究人员提供了未来挑战的基线性能水平。

### 接下来去哪里？

随着 SC2LE 的开放，DeepMind 和暴雪希望社区将有助于建立智能系统，人类可以认为是值得的星际争霸 2 对手。

未来的更新承诺消除游戏的简化，使它更像一个人如何玩，并访问更多的人类游戏重播，以帮助训练强化学习代理。

我一直是个游戏玩家。我小时候一直在玩江湖游戏。这种玩游戏的研究让我着迷。然而，打造世界上最好的游戏玩家并不是最让我兴奋的。

当一个智能系统能够学会将它从一个游戏中学到的原则应用到另一个游戏，甚至一个完全不同的环境，而不必重新开始时，它将获得真正的价值。

如果一个智能系统可以学习如何玩星际争霸 2，它还可以学习解决其他什么问题？

对于那些希望了解 SC2LE 更多信息的人，你可以在 DeepMind 的博客上阅读更多关于完整版本的信息，Siraj Raval 在他的 YouTube 频道上有一个关于如何开始使用 SC2LE 的很棒的介绍视频。

DeepMind 正在接受让我早上想起床的挑战。在我写这篇文章的时候，他们发布了一篇关于 AlphaGo Zero 的论文，这是 AlphaGo 迄今为止最先进的版本，可以在没有人工干预的情况下学习围棋。

我将在未来几周内解构 AlphaGo Zero，如果你有兴趣了解更多，请务必关注我。

如果你愿意加入我解构智能的使命，我每周在 [YouTube](https://www.youtube.com/channel/UCr8O8l5cCX85Oem1d18EezQ) 上发布一个视频，记录我在[自创人工智能硕士学位](https://medium.com/@mrdbourke/my-self-created-ai-masters-degree-ddc7aae92d0e)的旅程。

#### 对我有建议或者学习 AI？我很想收到你的来信！

**打招呼:[YouTube](https://www.youtube.com/channel/UCr8O8l5cCX85Oem1d18EezQ)|[Twitter](http://www.twitter.com)|[Email](mailto:daniel@mrdbourke.com)|[GitHub](http://www.github.com/mrdbourke)|[Patreon](http://www.patreon.com/mrdbourke)**